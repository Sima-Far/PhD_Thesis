{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epidemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Function_network.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epi_prmthr(beta_rate, step_n):\n",
    "    global beta, step\n",
    "    beta= beta_rate\n",
    "    step= step_n\n",
    "    \n",
    "    \n",
    "def SI_list_nodes_each_step(Graph, initial_infecteds):\n",
    "    G=copy.deepcopy(Graph)\n",
    "    #beta=0.1#0.001#0.02085574#0.00042337#0.00084674     #missinglink #0.00006864988#0.0006864988 #0.001144#0.00134228   #0.001121#0.002242           # weight_un=#0.00095969#0.00191938 #0.00136986# previous: 0.0027397\n",
    "    infected = initial_infecteds.copy()\n",
    "    N=G.order()\n",
    "    i= step #number of steps of iteration\n",
    "    susceptible = list(G.nodes())\n",
    "    \n",
    "    for infected_node in infected:\n",
    "        if infected_node in susceptible: \n",
    "            susceptible.remove(infected_node)\n",
    "    \n",
    "  \n",
    "    infected_node_list=[]\n",
    "\n",
    "    #Inicializar el tiempo\n",
    "    j=1\n",
    "    while j<i:\n",
    "        infected_1 = list()\n",
    "        #Realizar la infección\n",
    "        for node in infected: \n",
    "            if len(set(G.neighbors(node)))>0:\n",
    "                '''strategy here is that every time the first chance is get to the highest weight of edges in node neighbors. \n",
    "                if rhe neighbor with highest weight does not infect then the second chance is based on random sample choice'''\n",
    "                neigh = list(G.neighbors(node))\n",
    "                if len(neigh)==1:\n",
    "                    nei=neigh[0]\n",
    "                    #neigh=set(G.neighbors(node))\n",
    "                    if  nei not in list(infected):\n",
    "                        if  nei not in list(infected_1):\n",
    "                            if random.random()<1-((1-beta)**G[node][nei]['weight']):\n",
    "                                infected_1.append(nei)\n",
    "                # if len(edges) > 0: #some nodes have zero edges going into it #this one could be useful in directed network\n",
    "                else:\n",
    "                    for nbr in set(G.neighbors(node)):\n",
    "                        if nbr not in list(infected):\n",
    "                            if  nbr not in list(infected_1):\n",
    "                                if random.random()<1-((1-beta)**G[node][nbr]['weight']):\n",
    "                                    infected_1.append(nbr)\n",
    "        infected_node_list.append((j, infected_1))\n",
    "        #Remover a los infectados de los susceptibles:\n",
    "        infected += infected_1\n",
    "        for infected_node in infected_1:\n",
    "            if infected_node in susceptible: \n",
    "                susceptible.remove(infected_node)\n",
    "            \n",
    "\n",
    "        #Registrar el porcentaje de nodos infectados en una lista para luego graficar\n",
    "        \n",
    "        \n",
    "        #Actualizar el tiempo\n",
    "        j +=1\n",
    "                            \n",
    "    return infected_node_list\n",
    "\n",
    "def My_Epi_SI(Graph, initial_infecteds):\n",
    "    G=copy.deepcopy(Graph)\n",
    "    #beta= beta#0.001#0.02085574#0.00042337#0.00084674     #missinglink #0.00006864988#0.0006864988 #0.001144#0.00134228   #0.001121#0.002242           # weight_un=#0.00095969#0.00191938 #0.00136986# previous: 0.0027397\n",
    "    infected = initial_infecteds.copy()\n",
    "    N=G.order()\n",
    "    i= step+1 #number of steps of iteration\n",
    "    susceptible = list(G.nodes())\n",
    "    \n",
    "    for infected_node in infected:\n",
    "        if infected_node in susceptible: \n",
    "            susceptible.remove(infected_node)\n",
    "    \n",
    "    num_infected_results=[]\n",
    "    num_j=[]\n",
    "    num_j.append([0,float(len(infected)/N)])\n",
    "\n",
    "    #Inicializar el tiempo\n",
    "    t = 0\n",
    "    j=1\n",
    "    while j<i:\n",
    "        infected_1 = list()\n",
    "        #Realizar la infección\n",
    "        for node in infected: \n",
    "            if len(set(G.neighbors(node)))>0:\n",
    "                '''strategy here is that every time the first chance is get to the highest weight of edges in node neighbors. \n",
    "                if rhe neighbor with highest weight does not infect then the second chance is based on random sample choice'''\n",
    "                neigh = list(G.neighbors(node))\n",
    "                if len(neigh)==1:\n",
    "                    nei=neigh[0]\n",
    "                    #neigh=set(G.neighbors(node))\n",
    "                    if  nei not in list(infected):\n",
    "                        if  nei not in list(infected_1):\n",
    "                            if random.random()<1-((1-beta)**G[node][nei]['weight']):\n",
    "                                infected_1.append(nei)\n",
    "                # if len(edges) > 0: #some nodes have zero edges going into it #this one could be useful in directed network\n",
    "                else:\n",
    "                    for nbr in set(G.neighbors(node)):\n",
    "                        if nbr not in list(infected):\n",
    "                            if  nbr not in list(infected_1):\n",
    "                                if random.random()<1-((1-beta)**G[node][nbr]['weight']):\n",
    "                                    infected_1.append(nbr)\n",
    "        #Remover a los infectados de los susceptibles:\n",
    "        infected += infected_1\n",
    "        for infected_node in infected_1:\n",
    "            if infected_node in susceptible: \n",
    "                susceptible.remove(infected_node)\n",
    "            \n",
    "\n",
    "        #Registrar el porcentaje de nodos infectados en una lista para luego graficar\n",
    "\n",
    "        num_j.append([j,float(len(infected)/N)])\n",
    "        \n",
    "        #Actualizar el tiempo\n",
    "        j +=1\n",
    "                            \n",
    "    return num_j\n",
    "\n",
    "def multiran_result_SI_multiple_IN_IN(Graph, IN_IN_multiple):\n",
    "    dfs_un=pd.DataFrame(list(range(0,step+1)), columns=['time'])\n",
    "    for i in range(len(IN_IN_multiple)):\n",
    "        IN_IN= IN_IN_multiple[i]\n",
    "        I = My_Epi_SI(Graph,IN_IN)\n",
    "        df_un=pd.DataFrame([b[1] for b in I], columns=['vals'])\n",
    "        dfs_un.insert(i, f\"vals_{i}\", df_un) \n",
    "    return dfs_un\n",
    "\n",
    "def mean_max_min(dfs_un):\n",
    "    df_all_un = dfs_un.set_index('time')\n",
    "    mean_val_un = df_all_un.iloc[:,0:].mean(axis=1)\n",
    "    std_val_un= df_all_un.iloc[:,0:].std(axis=1)\n",
    "    min_val_un = mean_val_un - 2*std_val_un\n",
    "    max_val_un = mean_val_un + 2*std_val_un\n",
    "    return ( mean_val_un, max_val_un, min_val_un)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIK Epidemic - Epidemic Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epi_SIK_Model_forThreshold(Graph,initial_infecteds,beta,mu, step):\n",
    "     \n",
    "    G=copy.deepcopy(Graph)\n",
    "    infected = initial_infecteds.copy()\n",
    "    killed=[]\n",
    "    N=G.order()\n",
    "    i= step #number of steps of iteration\n",
    "    susceptible = list(G.nodes())\n",
    "    \n",
    "    for infected_node in infected:\n",
    "        if infected_node in susceptible: \n",
    "            susceptible.remove(infected_node)\n",
    "    \n",
    "    num_infected_results=[]\n",
    "    \n",
    "    num_killed=[]\n",
    "    killed=[]\n",
    "    num_killed.append((0,0))\n",
    "    num_infected_results.append((0,float(len(infected)/N)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Inicializar el tiempo\n",
    "    j=1\n",
    "    while j<i:\n",
    "        infected_1 = list()\n",
    "        #Realizar la infección\n",
    "        for node in infected: \n",
    "            if len(set(G.neighbors(node)))>0:\n",
    "                '''strategy here is that every time the first chance is get to the highest weight of edges in node neighbors. \n",
    "                if rhe neighbor with highest weight does not infect then the second chance is based on random sample choice'''\n",
    "                neigh = list(G.neighbors(node))\n",
    "                if len(neigh)==1:\n",
    "                    nei=neigh[0]\n",
    "                    #neigh=set(G.neighbors(node))\n",
    "                    if  nei not in list(infected):\n",
    "                        if  nei not in list(infected_1):               #neigh=set(G.neighbors(node))\n",
    "                            if random.random()<1-((1-beta)**G[node][nei]['weight']):\n",
    "                                infected_1.append(nei)\n",
    "                # if len(edges) > 0: #some nodes have zero edges going into it #this one could be useful in directed network\n",
    "                else:\n",
    "                    for nbr in set(G.neighbors(node)):\n",
    "                        if nbr not in list(infected):\n",
    "                            if  nbr not in list(infected_1):\n",
    "                                if random.random()<1-((1-beta)**G[node][nbr]['weight']):\n",
    "                                    infected_1.append(nbr)\n",
    "            #recover process\n",
    "            if random.random()<mu:\n",
    "                killed.append(node)\n",
    "                \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        #Remover a los infectados de los susceptibles:\n",
    "        infected += infected_1\n",
    "        for node in killed:\n",
    "            if node in infected: \n",
    "                infected.remove(node)\n",
    "            \n",
    "        for infected_node in infected:\n",
    "            if infected_node in susceptible: \n",
    "                susceptible.remove(infected_node)\n",
    "            \n",
    "            \n",
    "\n",
    "        #Registrar el porcentaje de nodos infectados en una lista para luego graficar\n",
    "        num_infected_results.append((j,float(len(infected)/N)))\n",
    "        num_killed.append((j,float(len(killed)/N)))\n",
    "        #Actualizar el tiempo\n",
    "        j += 1\n",
    "                          \n",
    "    return num_infected_results, num_killed\n",
    "\n",
    "\n",
    "\n",
    "def infected_fraction_gamma(Graph, initial_infecteds,beta_range, mu, step):\n",
    "     \n",
    "    '''gama=np.arange(0, 1, 0.02).tolist()\n",
    "    beta_range=[i * mu for i in gama]\n",
    "    totall_number_of_nodes=Graph.order()'''  # اگه اینجوری بنویسیم با افزایش میو بتا افزایش پیدا میکنه در ضورتی که باید برعکس یاشه دلیاشم اینه که باید بتا و میو رو فیکس کنیم ازش گاما بدست بیاریم نه اینکه گاما فیکس باشه\n",
    "    \n",
    "    #to wee the epidemic threshold we just need the epidemic behaviour of network in small gamma\n",
    "    #so the beta range does not need to go to 1\n",
    "    #select a small range unless your program will take a long time\n",
    "    #gamma_range=np.arange(0.05, 0.25, 0.05).tolist()\n",
    "    totall_number_of_nodes=Graph.order()\n",
    "    \n",
    "#run epi_SIK for a range of beta on the network(original and missing link)\n",
    "    infected_cases_gamma=[]\n",
    "    #infected_cases_gamma.append(float(len(initial_infecteds)/totall_number_of_nodes))\n",
    "\n",
    "#infection process\n",
    "    for b in beta_range:\n",
    "        #b=round(g*mu,3)\n",
    "        #IN_list, Killed_list=epi_SIK_Model_forThreshold(Graph, initial_infecteds, b , mu, step)\n",
    "        IN_list, Killed_list= epi_SIK_Model_forThreshold_after_steps(Graph, initial_infecteds, b , mu, step)\n",
    "        infected_cases_gamma.append(IN_list[-1][1])\n",
    "        \n",
    "    return infected_cases_gamma\n",
    "\n",
    "\n",
    "\n",
    "def epidemic_threshold_multirun(graph,IN_list,beta_range, mu, step):\n",
    "    dfs_ep=pd.DataFrame(beta_range, columns=['beta'])\n",
    "    for i in range(len(IN_list)):\n",
    "        IN_IN= IN_list[i]\n",
    "        df_ep= infected_fraction_gamma(graph,IN_IN,beta_range,mu, step)\n",
    "        dfs_ep.insert(i, f\"vals_{i}\", df_ep) \n",
    "        #df_un=pd.DataFrame([b[1] for b in I], columns=['vals'])\n",
    "    return dfs_ep\n",
    "    \n",
    "    \n",
    "    \n",
    "def mean_max_min_SIK(dfs_un):\n",
    "    df_all_un = dfs_un.set_index('beta')\n",
    "    mean_val_un = df_all_un.iloc[:,0:].mean(axis=1)\n",
    "    std_val_un= df_all_un.iloc[:,0:].std(axis=1)\n",
    "    min_val_un = mean_val_un - 2*std_val_un\n",
    "    max_val_un = mean_val_un + 2*std_val_un\n",
    "    return (mean_val_un, max_val_un, min_val_un)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def epi_SIK_Model_forThreshold_after_steps(Graph,initial_infecteds,beta,mu, step):\n",
    "     \n",
    "    G=copy.deepcopy(Graph)\n",
    "    infected = initial_infecteds.copy()\n",
    "    N=G.order()\n",
    "    i= step #number of steps of iteration\n",
    "    susceptible = list(G.nodes())\n",
    "    \n",
    "    for infected_node in infected:\n",
    "        if infected_node in susceptible: \n",
    "            susceptible.remove(infected_node)\n",
    "    \n",
    "    num_infected_results=[]\n",
    "    \n",
    "    num_killed=[]\n",
    "    killed=[]\n",
    "    num_killed.append((0,0))\n",
    "    num_infected_results.append((0,float(len(infected)/N)))\n",
    "    \n",
    "    #Inicializar el tiempo\n",
    "    j=1\n",
    "    while j<i:\n",
    "        infected_1 = list()\n",
    "        #Realizar la infección\n",
    "        for node in infected: \n",
    "            if len(set(G.neighbors(node)))>0:\n",
    "                '''strategy here is that every time the first chance is get to the highest weight of edges in node neighbors. \n",
    "                if rhe neighbor with highest weight does not infect then the second chance is based on random sample choice'''\n",
    "                neigh = list(G.neighbors(node))\n",
    "                if len(neigh)==1:\n",
    "                    nei=neigh[0]\n",
    "                    #neigh=set(G.neighbors(node))\n",
    "                    if  nei not in list(infected):\n",
    "                        if  nei not in list(infected_1):               #neigh=set(G.neighbors(node))\n",
    "                            if random.random()<1-((1-beta)**G[node][nei]['weight']):\n",
    "                                infected_1.append(nei)\n",
    "                # if len(edges) > 0: #some nodes have zero edges going into it #this one could be useful in directed network\n",
    "                else:\n",
    "                    for nbr in set(G.neighbors(node)):\n",
    "                        if nbr not in list(infected):\n",
    "                            if  nbr not in list(infected_1):\n",
    "                                if random.random()<1-((1-beta)**G[node][nbr]['weight']):\n",
    "                                    infected_1.append(nbr)\n",
    "            #recover process\n",
    "            if j%5==0:\n",
    "                if random.random()<mu:\n",
    "                    killed.append(node)\n",
    "                            \n",
    "                    \n",
    "        #Remover a los infectados de los susceptibles:\n",
    "        infected += infected_1\n",
    "        for node in killed:\n",
    "            if node in infected: \n",
    "                infected.remove(node)\n",
    "            \n",
    "        for infected_node in infected:\n",
    "            if infected_node in susceptible: \n",
    "                susceptible.remove(infected_node)\n",
    "            \n",
    "            \n",
    "\n",
    "        #Registrar el porcentaje de nodos infectados en una lista para luego graficar\n",
    "        num_infected_results.append((j,float(len(infected)/N)))\n",
    "        num_killed.append((j,float(len(killed)/N)))\n",
    "        #Actualizar el tiempo\n",
    "        j += 1\n",
    "                          \n",
    "    return num_infected_results, num_killed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ُSIR Epidemic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epi_SIR_Model_forThreshold_after_steps(Graph,initial_infecteds,beta,mu,step):\n",
    " \n",
    "    G=copy.deepcopy(Graph)\n",
    "    infected = initial_infecteds.copy()\n",
    "    \n",
    "    N=G.order()\n",
    "    i= step #number of steps of iteration\n",
    "    susceptible = list(G.nodes())\n",
    "    \n",
    "    for infected_node in infected:\n",
    "        if infected_node in susceptible: \n",
    "            susceptible.remove(infected_node)\n",
    "    \n",
    "    num_infected_results=[]\n",
    "    \n",
    "    num_killed=[]\n",
    "    killed=[]\n",
    "    num_killed.append((0,0))\n",
    "    num_infected_results.append((0,float(len(infected)/N)))\n",
    "    \n",
    "    #Inicializar el tiempo\n",
    "    j=1\n",
    "    while j<i:\n",
    "        infected_1 = list()\n",
    "        #Realizar la infección\n",
    "        for node in infected: \n",
    "            if len(set(G.neighbors(node)))>0:\n",
    "                '''strategy here is that every time the first chance is get to the highest weight of edges in node neighbors. \n",
    "                if rhe neighbor with highest weight does not infect then the second chance is based on random sample choice'''\n",
    "                neigh = list(G.neighbors(node))\n",
    "                if len(neigh)==1:\n",
    "                    nei=neigh[0]\n",
    "                    #neigh=set(G.neighbors(node))\n",
    "                    if  nei not in list(infected):\n",
    "                        if  nei not in list(infected_1):               #neigh=set(G.neighbors(node))\n",
    "                            if random.random()<1-((1-beta)**G[node][nei]['weight']):\n",
    "                                infected_1.append(nei)\n",
    "                # if len(edges) > 0: #some nodes have zero edges going into it #this one could be useful in directed network\n",
    "                else:\n",
    "                    for nbr in set(G.neighbors(node)):\n",
    "                        if nbr not in list(infected):\n",
    "                            if  nbr not in list(infected_1):\n",
    "                                if random.random()<1-((1-beta)**G[node][nbr]['weight']):\n",
    "                                    infected_1.append(nbr)\n",
    "            #recover process\n",
    "            if j%5==0:\n",
    "                if random.random()<mu:\n",
    "                    killed.append(node)\n",
    "                    susceptible.append(node)\n",
    "                    \n",
    "                            \n",
    "                    \n",
    "        #Remover a los infectados de los susceptibles:\n",
    "        infected += infected_1\n",
    "        for node in killed:\n",
    "            if node in infected: \n",
    "                infected.remove(node)\n",
    "            \n",
    "        for infected_node in infected:\n",
    "            if infected_node in susceptible: \n",
    "                susceptible.remove(infected_node)\n",
    "            \n",
    "            \n",
    "\n",
    "        #Registrar el porcentaje de nodos infectados en una lista para luego graficar\n",
    "        num_infected_results.append((j,float(len(infected)/N)))\n",
    "        num_killed.append((j,float(len(killed)/N)))\n",
    "        #Actualizar el tiempo\n",
    "        j += 1\n",
    "                          \n",
    "    return num_infected_results, num_killed\n",
    "\n",
    "def infected_fraction_gamma_SIR(Graph, initial_infecteds,beta_range, mu, step):\n",
    "\n",
    "    totall_number_of_nodes=Graph.order()\n",
    "    \n",
    "    #run epi_SIK for a range of beta on the network(original and missing link)\n",
    "    infected_cases_gamma=[]\n",
    "    #infected_cases_gamma.append(float(len(initial_infecteds)/totall_number_of_nodes))\n",
    "\n",
    "    #infection process\n",
    "    for b in beta_range:\n",
    "        IN_list, Killed_list= epi_SIR_Model_forThreshold_after_steps(Graph, initial_infecteds, b , mu, step)\n",
    "        infected_cases_gamma.append(IN_list[-1][1])\n",
    "        \n",
    "    return infected_cases_gamma\n",
    "\n",
    "def epidemic_threshold_multirun_SIR(graph,IN_list,beta_range, mu, step):\n",
    "    dfs_ep=pd.DataFrame(beta_range, columns=['beta'])\n",
    "    for i in range(len(IN_list)):\n",
    "        IN_IN= IN_list[i]\n",
    "        df_ep= infected_fraction_gamma_SIR(graph,IN_IN,beta_range,mu, step)\n",
    "        dfs_ep.insert(i, f\"vals_{i}\", df_ep) \n",
    "        #df_un=pd.DataFrame([b[1] for b in I], columns=['vals'])\n",
    "    return dfs_ep\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epidemic - Multi edge graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def My_Epi_SI_MultiDigraph_risk_one_moveReason_allow(Graph, initial_infecteds, mov_reason):\n",
    "    G=copy.deepcopy(Graph)\n",
    "    beta=0.001#0.001#0.02085574#0.00042337#0.00084674     #missinglink #0.00006864988#0.0006864988 #0.001144#0.00134228   #0.001121#0.002242           # weight_un=#0.00095969#0.00191938 #0.00136986# previous: 0.0027397\n",
    "    infected = initial_infecteds.copy()\n",
    "    N=G.order()\n",
    "    market=[b'AGLOMERACAO COM FINALIDADE COMERCIAL',b'AGLOMERACAO SEM FINALIDADE COMERCIAL',b'RETORNO DE AGLOMERACAO']\n",
    "    t=101 #number of steps of iteration\n",
    "    susceptible = list(G.nodes())\n",
    "    \n",
    "    for infected_node in infected:\n",
    "        if infected_node in susceptible: \n",
    "            susceptible.remove(infected_node)\n",
    "    \n",
    "    num_j=[]\n",
    "    num_j.append([0,float(len(infected)/N)])\n",
    "    #Inicializar el tiempo\n",
    "    j=1\n",
    "    while j<t:\n",
    "        infected_1 = list()\n",
    "        #Realizar la infección\n",
    "        for node in infected: \n",
    "            if len(set(G.neighbors(node)))>0:\n",
    "                '''strategy here is that every time the first chance is get to the highest weight of edges in node neighbors. \n",
    "                if rhe neighbor with highest weight does not infect then the second chance is based on random sample choice'''\n",
    "                neigh = list(G.neighbors(node))\n",
    "                if len(neigh)==1:\n",
    "                    nei=neigh[0]\n",
    "                    #neigh=set(G.neighbors(node))\n",
    "                    if  nei not in list(infected):\n",
    "                        if  nei not in list(infected_1):\n",
    "                            dict_multiedges=G.get_edge_data(node, nei)\n",
    "                            for i in range(len(dict_multiedges)):\n",
    "                                if  nei not in list(infected_1):\n",
    "                                    if mov_reason=='Livestock_Market':\n",
    "                                        if G[node][nei][i]['mov_reason'] in market:\n",
    "                                            if random.random()<1-((1-beta)**G[node][nei][i]['weight']):\n",
    "                                                infected_1.append(nei)   \n",
    "                                    else:\n",
    "                                        if G[node][nei][i]['mov_reason']==mov_reason:\n",
    "                                            if random.random()<1-((1-beta)**G[node][nei][i]['weight']):\n",
    "                                                infected_1.append(nei)\n",
    "                # if len(edges) > 0: #some nodes have zero edges going into it #this one could be useful in directed network\n",
    "                else:\n",
    "                    for nbr in set(G.neighbors(node)):\n",
    "                        if nbr not in list(infected):\n",
    "                            if  nbr not in list(infected_1):\n",
    "                                dict_multiedges=G.get_edge_data(node, nbr)\n",
    "                                for i in range(len(dict_multiedges)):\n",
    "                                    if  nbr not in list(infected_1):\n",
    "                                        if mov_reason=='Livestock_Market':\n",
    "                                            if G[node][nbr][i]['mov_reason'] in market:\n",
    "                                                if random.random()<1-((1-beta)**G[node][nbr][i]['weight']):\n",
    "                                                    infected_1.append(nbr)   \n",
    "                                        else:\n",
    "                                            if G[node][nbr][i]['mov_reason']==mov_reason:\n",
    "                                                if random.random()<1-((1-beta)**G[node][nbr][i]['weight']):\n",
    "                                                    infected_1.append(nbr)\n",
    "                           \n",
    "        #Remover a los infectados de los susceptibles:\n",
    "        infected += infected_1\n",
    "        for infected_node in infected_1:\n",
    "            if infected_node in susceptible: \n",
    "                susceptible.remove(infected_node)\n",
    "            \n",
    "\n",
    "        #Registrar el porcentaje de nodos infectados en una lista para luego graficar\n",
    "        num_j.append([j,float(len(infected)/N)])\n",
    "        \n",
    "        #Actualizar el tiempo\n",
    "        j +=1\n",
    "                            \n",
    "    return num_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make df include infection step for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_net_city_infection_step(G, initial_infecteds):\n",
    "    \n",
    "    Infected_node_list_initial_west= SI_list_nodes_each_step(G,initial_infecteds)\n",
    "    \n",
    "    in_city_in_step=gpd.read_file('mg_gps_map_data/mg_city/31MUE250GC_SIR.shp')\n",
    "    in_city_in_step['city_number']= in_city_in_step.index\n",
    "    in_city_in_step['infection_step']=None\n",
    "    for node in initial_infecteds:\n",
    "        i=in_city_in_step[in_city_in_step['city_number']==node].index.values\n",
    "    #if in_city_in_step.loc[i,'infection_step'] is True: it does not work like this. because python is khar and does not understand\n",
    "                                                        # this and consider in_city_in_step.loc[i,'infection_step'] as an aaray not an element\n",
    "                                                        # so you should make an element from it like the thing in following line\n",
    "        if list(in_city_in_step.loc[i,'infection_step'])[0] is None:\n",
    "            in_city_in_step.loc[i,'infection_step']=0\n",
    "    for item in Infected_node_list_initial_west:\n",
    "        infection_step=item[0]\n",
    "        for node in item[1]:\n",
    "            i= in_city_in_step[in_city_in_step['city_number']==node].index.values\n",
    "            if list(in_city_in_step.loc[i,'infection_step'])[0] is None:\n",
    "                in_city_in_step.loc[i,'infection_step']=infection_step\n",
    "    \n",
    "    #***************you should do this because ther is 2 types of None/Nan in the dataset\n",
    "    #one None for the cities that are not infected and do not have infection-step\n",
    "    #one for the cities that do not have premises in this network size\n",
    "    none=gpd.GeoDataFrame()\n",
    "    for i in range(853):\n",
    "        if in_city_in_step.loc[i,'infection_step']!=in_city_in_step.loc[i,'infection_step']:\n",
    "            none=none.append(dc.loc[i])\n",
    "    \n",
    "    '''#maximum infection step we let infection diffuse\n",
    "    #min_max=[0,20]\n",
    "    #fig, ax = plt.subplots(1, 1, figsize=[10,10])\n",
    "\n",
    "    #in_city_in_step.plot(column='infection_step',edgecolor='black',linewidth=0.2, cmap='RdYlGn',\\\n",
    "     #          missing_kwds={'color': 'green', \"label\": \"Not Infected\"}, ax=ax,\\\n",
    "      #                vmin=min_max[0],vmax=min_max[-1])#'RdYlGn'\n",
    "    #none.plot(color='black', ax=ax, label='withhout premises')\n",
    "    #cax = fig.add_axes([1, 0.2, 0.03, 0.5])\n",
    "    #sm=plt.cm.ScalarMappable(cmap='autumn',norm=plt.Normalize(vmin=min_max[0],\n",
    "    #                                                    vmax=min_max[-1]))#'RdYlGn'\n",
    "    #sm._A = []\n",
    "    #cbr = fig.colorbar(sm, cax=cax)\n",
    "    #cbr.set_label('infection step',fontsize=15, labelpad=20)\n",
    "    #cbr.ax.tick_params(labelsize=28)\n",
    "    #ax.set_axis_off()\n",
    "    \n",
    "    \n",
    "    #plt.savefig('riskmap_pic_video/paper/map_ratio_Instep_risk_INwest'+str(string_un_type)+str(d-1)+'.pdf',format=\"PDF\",bbox_inches='tight')'''\n",
    "    \n",
    "    return in_city_in_step, none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add uncertainty - weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_uncertainty_weight(G, a):\n",
    "    Gw= copy.deepcopy(G)\n",
    "    for (u,v) in Gw.edges():\n",
    "        Gw[u][v]['weight']+=int(Gw[u][v]['weight']*(a/100))\n",
    "    return Gw\n",
    "\n",
    "'''***chechdef add_uncertainty_weight_movement_reason_multiDigraph(G, mov_reason):\n",
    "    a=20\n",
    "    for (u,v) in G.edges(): #be careful it should not be .edges(data=True)\n",
    "        dict_multiedges=G.get_edge_data(u, v)\n",
    "        for i in range(len(dict_multiedges)):\n",
    "            if G[u][v][i]['mov_reason']==mov_reason:\n",
    "                G[u][v][i]['weight']+=int(G[u][v][i]['weight']*(a/100))            \n",
    "    return G'''\n",
    "\n",
    "def add_just_missing_link_entire(Graph,a):\n",
    "    list_weights=[]\n",
    "    for (u,v) in Graph.edges():\n",
    "        list_weights.append(Graph.adj[u][v]['weight'])\n",
    "    G_o=copy.deepcopy(Graph)\n",
    "    G1=copy.deepcopy(Graph)\n",
    "    N_missing_link=int((a*0.01)*G1.number_of_edges())\n",
    "    i=0\n",
    "    while i<N_missing_link:\n",
    "        node_tuple=random.sample(list(G1.nodes()), 2)\n",
    "\n",
    "        if (node_tuple[0],node_tuple[1]) not in G1.edges():\n",
    "            #first find the mean of weights of edges this node have\n",
    "           #be careful if you first add new edge you will get erros in calculating mean of weight of edges\n",
    "            list_neigh=list(G_o.edges(node_tuple[0],data=True))#make a list of weights of the edges incident to this node in the original network without missing link\n",
    "            list_w=[item[2]['weight'] for item in list_neigh]\n",
    "            G1.add_edge(node_tuple[0],node_tuple[1])\n",
    "            #If node in original network do not have any edges and it is isolated\n",
    "            if len(list_neigh)==0:\n",
    "                G1[node_tuple[0]][node_tuple[1]]['weight']=list(random.sample(list_weights, 1))[0]\n",
    "            #if node has neighbors we choose weight for missing link based on weights of other edges of node\n",
    "            else:\n",
    "                G1.adj[node_tuple[0]][node_tuple[1]]['weight']=list(random.sample(list_w, 1))[0]\n",
    "                \n",
    "        \n",
    "            # this one should be select based on the distribution for the weight of the network was used\n",
    "            i+=1\n",
    "        \n",
    "    return G1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate the distances between cities and microregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add missing linkd bases on distance based on cities\n",
    "def list_distance_cities():\n",
    "    mg_map=gpd.read_file('mg_gps_map_data/mg_city/31MUE250GC_SIR.shp')\n",
    "    mg_map['city_number'] = mg_map.index\n",
    "    mg_map['center_co']=mg_map['geometry'].centroid\n",
    "    mg_map_cn=list(mg_map['city_number'])\n",
    "    distance=[]\n",
    "    for t in range(len(mg_map)):\n",
    "        co1=mg_map.loc[t,'center_co']\n",
    "        coords_1=(co1.x,co1.y)\n",
    "        for j in range(t , len(mg_map)):\n",
    "            co2=mg_map.loc[j,'center_co']\n",
    "            coords_2=(co2.x,co2.y)\n",
    "            d=geopy.distance.distance(coords_1, coords_2).kilometers\n",
    "            if d!=0:\n",
    "                distance.append((mg_map.loc[t,'city_number'],mg_map.loc[j,'city_number'],d))#len(distance)= 853*852/2\n",
    "    return distance\n",
    "\n",
    "#add missing linkd bases on distance based on cities\n",
    "def list_distance_microregions():\n",
    "    mg_micro=gpd.read_file('mg_gps_map_data/mcrrgn_mg/31MI2500G.shp')\n",
    "    mg_micro['center_co']=mg_micro['geometry'].centroid\n",
    "    #we should match the names to the names we have in the nodes attribure /some of them have capital letter come lower letter\n",
    "\n",
    "    micro_names=list(mg_micro['nome'])\n",
    "    distance=[]\n",
    "    for t in range(len(mg_micro)):\n",
    "        co1=mg_micro.loc[t,'center_co']\n",
    "        coords_1=(co1.x,co1.y)\n",
    "        for j in range(t, len(mg_micro)):\n",
    "            co2=mg_micro.loc[j,'center_co']\n",
    "            coords_2=(co2.x,co2.y)\n",
    "            d=geopy.distance.distance(coords_1, coords_2).kilometers\n",
    "            if d!=0:\n",
    "                distance.append((mg_micro.loc[t,'nome'],mg_micro.loc[j,'nome'],d))\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################calculate probability based on inverse distance###################\n",
    "def prob_inverse_dist_microregion():\n",
    "    distance= list_distance_microregions()\n",
    "    s = sum([1/e[2] for e in distance])\n",
    "    list_prob=[(e[0] , e[1] , (1/e[2])/s) for e in distance]\n",
    "    #note: this list is compatible to the siituation that if distance=0 means the same microregion picked the probability\n",
    "    #is zero. if we have a probability for the transactions in same microregion as P-s. we need to use ((1/e[2])/s)*(1-p)\n",
    "    #instead of (1/e[2])/s.\n",
    "    \n",
    "    return list_prob\n",
    "\n",
    "\n",
    "def prob_same_transaction_microregion(data_part):\n",
    "    transaction=[]\n",
    "    for i in range(len(data_part)):\n",
    "        transaction.append((data_part.loc[i,'oigin_microregion_name'],data_part.loc[i,'destination_microregion_name']))\n",
    "    \n",
    "    cs=0\n",
    "    cu=0\n",
    "    for i in range(len(transaction)):\n",
    "        if transaction[i][0]==transaction[i][1]:\n",
    "            cs+=1\n",
    "        else:\n",
    "            cu+=1\n",
    "    p_same_area=cs/(cs+cu)\n",
    "    \n",
    "    return p_same_area\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add uncertainty - missing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_just_missing_link_entire(Graph,a):\n",
    "    list_weights=[]\n",
    "    for (u,v) in Graph.edges():\n",
    "        list_weights.append(Graph.adj[u][v]['weight'])\n",
    "    G_o=copy.deepcopy(Graph)\n",
    "    G1=copy.deepcopy(Graph)\n",
    "    N_missing_link=int((a*0.01)*G1.number_of_edges())\n",
    "    i=0\n",
    "    while i<N_missing_link:\n",
    "        node_tuple=random.sample(list(G1.nodes()), 2)\n",
    "\n",
    "        if (node_tuple[0],node_tuple[1]) not in G1.edges():\n",
    "            #first find the mean of weights of edges this node have\n",
    "           #be careful if you first add new edge you will get erros in calculating mean of weight of edges\n",
    "            list_neigh=list(G_o.edges(node_tuple[0],data=True))#make a list of weights of the edges incident to this node in the original network without missing link\n",
    "            list_w=[item[2]['weight'] for item in list_neigh]\n",
    "            G1.add_edge(node_tuple[0],node_tuple[1])\n",
    "            #If node in original network do not have any edges and it is isolated\n",
    "            if len(list_neigh)==0:\n",
    "                G1[node_tuple[0]][node_tuple[1]]['weight']=list(random.sample(list_weights, 1))[0]\n",
    "            #if node has neighbors we choose weight for missing link based on weights of other edges of node\n",
    "            else:\n",
    "                G1.adj[node_tuple[0]][node_tuple[1]]['weight']=list(random.sample(list_w, 1))[0]\n",
    "                \n",
    "        \n",
    "            # this one should be select based on the distribution for the weight of the network was used\n",
    "            i+=1\n",
    "        \n",
    "    return G1\n",
    "\n",
    "\n",
    "\n",
    "def add_ML_distance_cities_net_city(G, a):#here we have distance between pair of nodes because nodes are cities\n",
    "    distance= list_distance_cities()\n",
    "    s = sum([1/e[2] for e in distance])\n",
    "    list_prob=[(e[0] , e[1] , (1/e[2])/s) for e in distance]\n",
    "    \n",
    "    #add missing link\n",
    "    #not all of the mesoregion\n",
    "    w1= nx.get_edge_attributes(G,'weight')\n",
    "    w_values=list(w1.values())\n",
    "    #add to the entire network 10%, 30%\n",
    "    n_edge=G.number_of_edges()\n",
    "    list_edges= G.edges() #set type\n",
    "    #entire 10%\n",
    "    e_ml=round(n_edge*(a/100))\n",
    "    ne=0\n",
    "    new_e= e_ml-ne\n",
    "    l_edge= []\n",
    "    while ne<e_ml:\n",
    "        l_e= random.choices([(j[0],j[1]) for j in list_prob],weights=[(j[2]) for j in list_prob],k= new_e)\n",
    "        l_e= set(l_e) - list_edges\n",
    "        l_edge = l_edge+ list(l_e)\n",
    "        ne= len(l_edge)\n",
    "        new_e= e_ml-ne\n",
    "\n",
    "    #randomely find the edges with the specific origin destination mesoregion - add them to network randomely. the missing link\n",
    "    #are choosed randomely not based on past dataset\n",
    "    G_distance=G.copy()\n",
    "    for element in l_edge:\n",
    "        w=random.sample(w_values,1)\n",
    "        a= random.randint(0,1)\n",
    "        #choose the direstion randomly\n",
    "        if a==0:\n",
    "            G_distance.add_edge(element[0], element[1], weight=w[0])\n",
    "        else:\n",
    "            G_distance.add_edge(element[1], element[0], weight=w[0])\n",
    "                \n",
    "    return G_distance\n",
    "                \n",
    "        \n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing links to network premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ML_distance_microregion_net_premise(G, data_part, a):#here we have distance between pair of nodes because nodes are cities\n",
    "    \n",
    "    list_prob= prob_inverse_dist_microregion()\n",
    "    p_same_area= prob_same_transaction_microregion(data_part)\n",
    "    dict_cnu_mirna= dict_cityNu_microNa(data_part)\n",
    "    micro_names=  list(dict_cnu_mirna.values())\n",
    "    \n",
    "    #make list of probability\n",
    "    mic_list_prob=[]\n",
    "    for t in range(len(micro_names)):\n",
    "        for j in range(len(micro_names)):\n",
    "            if micro_names[t]==micro_names[j]:\n",
    "                mic_list_prob.append((micro_names[t],micro_names[j],p_same_area))\n",
    "            else:\n",
    "                for el in  list_prob:\n",
    "                    if el[0]==micro_names[t]:\n",
    "                        if el[1]==micro_names[j]:\n",
    "                            if el[2]!=0:\n",
    "                                mic_list_prob.append((micro_names[t],micro_names[j],el[2] *(1-p_same_area)))\n",
    "                                \n",
    "\n",
    "    w1= nx.get_edge_attributes(G,'weight')\n",
    "    w_values=list(w1.values())\n",
    "    #add to the entire network 10%, 30%\n",
    "    n_edge=G.number_of_edges()\n",
    "    list_edges= G.edges() #set type\n",
    "    #entire 10%\n",
    "    e_ml=round(n_edge*(a/100))\n",
    "    \n",
    "    mic_l_edge=random.choices([(j[0],j[1]) for j in mic_list_prob],weights=[(j[2]) for j in mic_list_prob],k= e_ml)\n",
    "    #find distribution of trades in l_edg\n",
    "    T_1=set(mic_l_edge)\n",
    "    l_dist=list()\n",
    "    for x in T_1:\n",
    "        C=mic_l_edge.count(x)\n",
    "        l_dist.append([x,C])\n",
    "    #randomely find the edges with the specific origin destination mesoregion - add them to network randomely. the missing link\n",
    "    #are choosed randomely not based on past dataset\n",
    "    G_distance_mic=G.copy()\n",
    "    for element in l_dist:\n",
    "        nodes1 = [x for x,y in G.nodes(data=True) if dict_cnu_mirna[y['city_number']]== element[0][0]]     \n",
    "        nodes2 = [x for x,y in G.nodes(data=True) if dict_cnu_mirna[y['city_number']]== element[0][1]]\n",
    "        c=0\n",
    "        while c<element[1]:\n",
    "            n1=random.sample(nodes1,1)[0]\n",
    "            n2=random.sample(nodes2,1)[0]\n",
    "            if (n1,n2) not in list(G_distance_mic.edges()):\n",
    "                w=random.sample(w_values,1)\n",
    "                G_distance_mic.add_edge(n1, n2, weight=w[0])\n",
    "            c+=1\n",
    "\n",
    "          \n",
    "    return G_distance_mic\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ML_distance_cities_net_premise(G):#here we have distance between pair of nodes because nodes are cities\n",
    "    distance= list_distance_microregions()\n",
    "    s = sum([1/e[2] for e in distance])\n",
    "    list_prob=[(e[0] , e[1] , (1/e[2])/s) for e in distance]\n",
    "    \n",
    "    #add missing link\n",
    "    #not all of the mesoregion\n",
    "    w1= nx.get_edge_attributes(G,'weight')\n",
    "    w_values=list(w1.values())\n",
    "    #add to the entire network 10%, 30%\n",
    "    n_edge=G.number_of_edges()\n",
    "    list_edges= G.edges() #set type\n",
    "    #entire 10%\n",
    "    a= 25\n",
    "    e_ml=round(n_edge*(a/100))\n",
    "    ne=0\n",
    "    new_e= e_ml-ne\n",
    "    l_edge= []\n",
    "    while ne<e_ml:\n",
    "        l_e= random.choices([(j[0],j[1]) for j in list_prob],weights=[(j[2]) for j in list_prob],k= new_e)\n",
    "        l_e= set(l_e) - list_edges\n",
    "        l_edge = l_edge+ list(l_e)\n",
    "        ne= len(l_edge)\n",
    "        new_e= e_ml-ne\n",
    "\n",
    "    #randomely find the edges with the specific origin destination mesoregion - add them to network randomely. the missing link\n",
    "    #are choosed randomely not based on past dataset\n",
    "    G_distance=G.copy()\n",
    "    for element in l_edge:\n",
    "        w=random.sample(w_values,1)\n",
    "        a= random.randint(0,1)\n",
    "        #choose the direstion randomly\n",
    "        if a==0:\n",
    "            G_distance.add_edge(element[0], element[1], weight=w[0])\n",
    "        else:\n",
    "            G_distance.add_edge(element[1], element[0], weight=w[0])\n",
    "                \n",
    "    return G_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a shp dataframe with the value assigned to each city to plot\n",
    "def get_city_value_rate_shp_dtaframe(cities_rate):\n",
    "       #cities_rate is a list of tuple. each lists is a value and a list of cities with that value\n",
    "    mg_map= gpd.read_file('mg_gps_map_data/mg_city/31MUE250GC_SIR.shp')\n",
    "    mg_map['infection_step']=None\n",
    "    city_nc= np.load('mg_gps_map_data/dict_cityNunmber_to_cityCode.npy',allow_pickle='TRUE').item()\n",
    "    for item in cities_rate:\n",
    "        infection_step=item[0]\n",
    "        for node in item[1]:\n",
    "            code= city_nc[node]\n",
    "            i= mg_map[mg_map['CD_GEOCMU']==str(code)].index.values\n",
    "            if list(mg_map.loc[i,'infection_step'])[0] is None:\n",
    "                mg_map.loc[i,'infection_step']= infection_step\n",
    "\n",
    "    return mg_map\n",
    "\n",
    "def get_df_city_value_shp(cities_value):\n",
    "       #cities_rate is a list of tuple. each lists is a value and a list of cities with that value\n",
    "    mg_map= gpd.read_file('mg_gps_map_data/mg_city/31MUE250GC_SIR.shp')\n",
    "    mg_map['rate_value']= None\n",
    "    c_nc= np.load('mg_gps_map_data/dict_cityNunmber_to_cityCode.npy',allow_pickle='TRUE').item()\n",
    "    for item in cities_value:\n",
    "        value=item[1]\n",
    "        code= c_nc[item[0]]\n",
    "        i= mg_map[mg_map['CD_GEOCMU']==str(code)].index.values\n",
    "        mg_map.loc[i,'rate_value']= value\n",
    "        #mg_map.loc[item[0],'rate_value']= value\n",
    "\n",
    "    return mg_map\n",
    "\n",
    "\n",
    "def get_df_micro_name_value_shp(micro_value):\n",
    "    mg_map= gpd.read_file('mg_gps_map_data/mcrrgn_mg/31MI2500G.shp')\n",
    "    mg_map['rate_value']= None\n",
    "\n",
    "    for item in micro_value:\n",
    "        value=item[1]\n",
    "        mic_name= item[0]\n",
    "        i= mg_map[mg_map['nome']== mic_name].index.values\n",
    "        mg_map.loc[i,'rate_value']= value\n",
    "    \n",
    "    return mg_map\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate spatial correlation of a value in shp geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Morans_I(shp_map):\n",
    "        #replace the None with 20 in map_rate\n",
    "    map_rate_new= shp_map.fillna(value= 20)\n",
    "\n",
    "    w = libpysal.weights.Queen.from_dataframe(map_rate_new)\n",
    "\n",
    "# Calculate Moran's I for the two variables\n",
    "    moran1 = esda.moran.Moran(map_rate_new['rate_value'], w)\n",
    "    \n",
    "    return moran1.I"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
