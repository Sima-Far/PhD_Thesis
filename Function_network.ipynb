{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_dataset_between_twoday(data, first_day, end_day):\n",
    "    data_part=data.loc[first_day-1<data['DAY']]\n",
    "    data_part=data_part.loc[data_part['DAY']<end_day+1]\n",
    "    data_part=data_part.reset_index(drop=True)\n",
    "    \n",
    "    return data_part\n",
    "\n",
    "\n",
    "def cut_dataset_movement_reason(dataset, reason_transmission):\n",
    "    market=[\"b'AGLOMERACAO COM FINALIDADE COMERCIAL'\",\"b'AGLOMERACAO SEM FINALIDADE COMERCIAL'\",\"b'RETORNO DE AGLOMERACAO'\"]\n",
    "    if reason_transmission=='Livestock_Market': #because there is 3 purpose related to livestock market\n",
    "        data_part=dataset.loc[dataset['Finalidade'].isin(market)]\n",
    "    else:\n",
    "        data_part=dataset.loc[dataset['Finalidade']==reason_transmission]\n",
    "    data_part=data_part.reset_index(drop=True)\n",
    "    return data_part\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate network of premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_net_premises_cityn_attr_datapart(data_part, w_method): \n",
    "  \n",
    "    if w_method=='animal':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['Codigo_do_Local_de_Origem','Codigo_do_Local_de_Destino',\\\n",
    "                    'city_n_origin','city_n_destination']).agg(weight=('animais','sum')).reset_index())\n",
    "    if w_method=='movement':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['Codigo_do_Local_de_Origem','Codigo_do_Local_de_Destino',\\\n",
    "                    'city_n_origin','city_n_destination']).size().reset_index(name=\"weight\"))\n",
    "\n",
    "    G=nx.DiGraph()\n",
    "    for i in range(len(edges_list)):\n",
    "        G.add_edge(edges_list['Codigo_do_Local_de_Origem'][i],\\\n",
    "        edges_list['Codigo_do_Local_de_Destino'][i],\\\n",
    "                    weight=edges_list['weight'][i]) #weight animal\n",
    "        \n",
    "    attr={}\n",
    "    for j in range(len(edges_list)):\n",
    "        attr[edges_list.loc[j,'Codigo_do_Local_de_Origem']]={'city_number':edges_list.loc[j,'city_n_origin']}\n",
    "        attr[edges_list.loc[j,'Codigo_do_Local_de_Destino']]={'city_number':edges_list.loc[j,'city_n_destination']}\n",
    "        \n",
    "    nx.set_node_attributes(G, attr)\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate network of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_net_cities_weight_movement(dataframe, month_number, w_method, year_number):\n",
    "    #w_method means make weight from sum of the number of movement or from sum of the number of animal \n",
    "    dict_year_month_day_number={'2013':{1:(0,30),2:(31,58),3:(59,89),4:(90,119),5:(120,150),6:(151,180),7:(181,211),\\\n",
    "        8:(212,242),9:(243,272),10:(272,303),11:(304,333),12:(334,364)},'2014':{1:(365,395),2:(396,423),3:(424,454),\\\n",
    "                4:(455,484),5:(485,515),6:(516,545),7:(546,576),8:(577,607),9:(608,637),10:(638,668),11:(669,698),\\\n",
    "                12:(699,729)},'2015':{1:(730,760),2:(761,788),3:(789,819),4:(820,849),5:(850,880),6:(881,910),\\\n",
    "                7:(911,941),8:(941,972),9:(973,1002),10:(1003,1033),11:(1034,1063),12:(1064,1094)},'2016':{1:(1095,1125)\\\n",
    "                ,2:(1126,1154),3:(1155,1185),4:(1186,1215),5:(1216,1246),6:(1247,1276),7:(1277,1307),8:(1308,1338)\\\n",
    "                ,9:(1339,1368),10:(1369,1399),11:(1400,1429),12:(1430,1459)}}\n",
    "\n",
    "    #generate city_network\n",
    "    data_nl=dataframe[dataframe['local_non_local']=='non_local']\n",
    "    data_nl= data_nl.reset_index(drop=True)\n",
    "\n",
    "    month_day=dict_year_month_day_number[year_number][month_number]\n",
    "    #cut dataset for the 30 days for each month\n",
    "    #using function named cut_dataset_between_twoday\n",
    "    data_part= cut_dataset_between_twoday(data_nl, month_day[0], month_day[1])\n",
    "    \n",
    "    if w_method=='animal':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination'])\\\n",
    "                            .agg(weight=('animais','sum')).reset_index())\n",
    "    if w_method=='movement':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination'])\\\n",
    "                            .size().reset_index(name=\"weight\"))\n",
    "\n",
    "    G_cities=nx.DiGraph()\n",
    "    for i in range(len(edges_list)):\n",
    "        G_cities.add_edge(edges_list['city_n_origin'][i],\\\n",
    "        edges_list['city_n_destination'][i],\\\n",
    "                    weight=edges_list['weight'][i]) #weight animal\n",
    "    return G_cities\n",
    "\n",
    "\n",
    "\n",
    "def generate_net_cities_yearly(dataframe, w_method, year_number):\n",
    "    #w_method means make weight from sum of the number of movement or from sum of the number of animal \n",
    "    dict_year_month_day_number={'2013':{1:(0,30),2:(31,58),3:(59,89),4:(90,119),5:(120,150),6:(151,180),7:(181,211),\\\n",
    "        8:(212,242),9:(243,272),10:(272,303),11:(304,333),12:(334,364)},'2014':{1:(365,395),2:(396,423),3:(424,454),\\\n",
    "                4:(455,484),5:(485,515),6:(516,545),7:(546,576),8:(577,607),9:(608,637),10:(638,668),11:(669,698),\\\n",
    "                12:(699,729)},'2015':{1:(730,760),2:(761,788),3:(789,819),4:(820,849),5:(850,880),6:(881,910),\\\n",
    "                7:(911,941),8:(941,972),9:(973,1002),10:(1003,1033),11:(1034,1063),12:(1064,1094)},'2016':{1:(1095,1125)\\\n",
    "                ,2:(1126,1154),3:(1155,1185),4:(1186,1215),5:(1216,1246),6:(1247,1276),7:(1277,1307),8:(1308,1338)\\\n",
    "                ,9:(1339,1368),10:(1369,1399),11:(1400,1429),12:(1430,1459)}}\n",
    "\n",
    "    #generate city_network\n",
    "    data_nl=dataframe[dataframe['local_non_local']=='non_local']\n",
    "    data_nl= data_nl.reset_index(drop=True)\n",
    "\n",
    "    md1=dict_year_month_day_number[year_number][1]\n",
    "    md2=dict_year_month_day_number[year_number][12]\n",
    "    month_day=(md1[0],md2[1])\n",
    "    #cut dataset for the 30 days for each month\n",
    "    #using function named cut_dataset_between_twoday\n",
    "    data_part= cut_dataset_between_twoday(data_nl, month_day[0], month_day[1])\n",
    "    \n",
    "    if w_method=='animal':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination'])\\\n",
    "                            .agg(weight=('animais','sum')).reset_index())\n",
    "    if w_method=='movement':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination'])\\\n",
    "                            .size().reset_index(name=\"weight\"))\n",
    "\n",
    "    G_cities=nx.DiGraph()\n",
    "    for i in range(len(edges_list)):\n",
    "        G_cities.add_edge(edges_list['city_n_origin'][i],\\\n",
    "        edges_list['city_n_destination'][i],\\\n",
    "                    weight=edges_list['weight'][i]) #weight animal\n",
    "    return G_cities\n",
    "\n",
    "    \n",
    "def generate_net_cities_datapart(data_part, w_method): \n",
    "    #include none local datframe\n",
    "        #generate city_network\n",
    "    data_part=data_part[data_part['local_non_local']=='non_local']\n",
    "    data_nl= data_nl.reset_index(drop=True)\n",
    "    \n",
    "    if w_method=='animal':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination'])\\\n",
    "                            .agg(weight=('animais','sum')).reset_index())\n",
    "    if w_method=='movement':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination'])\\\n",
    "                            .size().reset_index(name=\"weight\"))\n",
    "\n",
    "    G_cities=nx.DiGraph()\n",
    "    for i in range(len(edges_list)):\n",
    "        G_cities.add_edge(edges_list['city_n_origin'][i],\\\n",
    "        edges_list['city_n_destination'][i],\\\n",
    "                    weight=edges_list['weight'][i]) #weight animal\n",
    "        \n",
    "\n",
    "    return G_cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make network of one type of movement reason from 6 biggest movement reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for make a network from sataset for one special movement reason\n",
    "\n",
    "def make_movement_reason_network(dataset,reason_transmission, weight_type):\n",
    "       \n",
    "    data_part= cut_dataset_movement_reason(dataset, reason_transmission)\n",
    "    #make network from this part of dataset include just one transmission reason\n",
    "    if weight_type== 'movement':\n",
    "        edges_list = pd.DataFrame(data_part.groupby(['Codigo_do_Local_de_Origem','Codigo_do_Local_de_Destino'])\\\n",
    "                                  .size().reset_index(name=\"weight\"))\n",
    "    if weight_type== 'animal':\n",
    "        edges_list = pd.DataFrame(data_part.groupby(['Codigo_do_Local_de_Origem','Codigo_do_Local_de_Destino'])\\\n",
    "                    .agg(weight=('animais','sum')).reset_index())\n",
    "    G = nx.from_pandas_edgelist(edges_list,'Codigo_do_Local_de_Origem','Codigo_do_Local_de_Destino', edge_attr='weight',create_using=nx.DiGraph())\n",
    "    return G\n",
    "\n",
    "def make_movement_reason_network_cities(dataset,reason_transmission, weight_type):\n",
    "\n",
    "    data_part= cut_dataset_movement_reason(dataset, reason_transmission)\n",
    "    #make network from this part of dataset include just one transmission reason\n",
    "    \n",
    "    if weight_type=='animal':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination'])\\\n",
    "                                .size().reset_index(name=\"weight\"))\n",
    "    if weight_type=='movement':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination'])\\\n",
    "                                                    .agg(weight=('animais','sum')).reset_index())\n",
    "\n",
    "    G = nx.from_pandas_edgelist(edges_list,'city_n_origin','city_n_destination', edge_attr='weight',create_using=nx.DiGraph())\n",
    "    return G\n",
    "\n",
    "\n",
    "def make_movement_reason_network_atr_cityn(dataset,reason_transmission, weight_type):\n",
    "    market=[\"b'AGLOMERACAO COM FINALIDADE COMERCIAL'\",\"b'AGLOMERACAO SEM FINALIDADE COMERCIAL'\",\"b'RETORNO DE AGLOMERACAO'\"]\n",
    "    if reason_transmission=='Livestock_Market': #because there is 3 purpose related to livestock market\n",
    "        data_part=dataset.loc[dataset['Finalidade'].isin(market)]\n",
    "    else:\n",
    "        data_part=dataset.loc[dataset['Finalidade']==reason_transmission]\n",
    "        \n",
    "    data_part= data_part.reset_index(drop=True)\n",
    "    #make network from this part of dataset include just one transmission reason\n",
    "    if weight_type== 'movement':\n",
    "        edges_list = pd.DataFrame(data_part.groupby(['Codigo_do_Local_de_Origem','Codigo_do_Local_de_Destino',\\\n",
    "                        'city_n_origin','city_n_destination'])\\\n",
    "                                  .size().reset_index(name=\"weight\"))\n",
    "    if weight_type== 'animal':\n",
    "        edges_list = pd.DataFrame(data_part.groupby(['Codigo_do_Local_de_Origem','Codigo_do_Local_de_Destino',\\\n",
    "                     'city_n_origin','city_n_destination'])\n",
    "                    .agg(weight=('animais','sum')).reset_index())\n",
    "        \n",
    "    G =nx.DiGraph()\n",
    "    for i in range(len(edges_list)):\n",
    "        G.add_edge(edges_list['Codigo_do_Local_de_Origem'][i],\\\n",
    "        edges_list['Codigo_do_Local_de_Destino'][i],\\\n",
    "                    weight=edges_list['weight'][i]) #weight animal\n",
    "     \n",
    "    attr={}\n",
    "    for j in range(len(edges_list)):\n",
    "        attr[edges_list.loc[j,'Codigo_do_Local_de_Origem']]={'city_number':edges_list.loc[j,'city_n_origin']}\n",
    "        attr[edges_list.loc[j,'Codigo_do_Local_de_Destino']]={'city_number':edges_list.loc[j,'city_n_destination']}\n",
    "        \n",
    "    nx.set_node_attributes(G, attr)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "def make_city_network_edge_label_movement_reason_MultiGraph(data_part, weight_type):\n",
    "    \n",
    "    if weight_type== 'movement':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination','Finalidade'])\\\n",
    "                                .size().reset_index(name=\"weight\"))\n",
    "    if weight_type== 'animal':\n",
    "        edges_list=pd.DataFrame(data_part.groupby(['city_n_origin','city_n_destination','Finalidade'])\\\n",
    "                                                    .agg(weight=('animais','sum')).reset_index())\n",
    "    \n",
    " \n",
    "    G=nx.MultiDiGraph() \n",
    "    \n",
    "    for i in range(len(edges_list)):\n",
    "        G.add_edge(edges_list['city_n_origin'][i],\\\n",
    "             edges_list['city_n_destination'][i], \\\n",
    "        weight=edges_list['weight'][i], mov_reason=edges_list['Finalidade'][i])\n",
    "        \n",
    "    \n",
    "    return G\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_dataframe(list_t):\n",
    "    #T_1SU=np.unique(list_t)  #if your list has none value this line will get error\n",
    "    T_1SU=set(list_t)\n",
    "    dist=[]# I use 'NM_MUNICIP' for column name to be similar to the column in MG geo data fram(dc)\n",
    "    for x in T_1SU:\n",
    "        C=list_t.count(x)\n",
    "        dist.append((x, C))\n",
    "    return dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pairlist_2nde_reverse(sub_li, sort_str):\n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order)\n",
    "    # key is set to sort using second element of \n",
    "    # sublist lambda has been used\n",
    "    if sort_str=='sort':\n",
    "        return(sorted(sub_li, key = lambda x: x[1])) \n",
    "    if sort_str=='reverse':\n",
    "        return(sorted(sub_li, key = lambda x: x[1], reverse=True)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccaed similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_set(list1, list2):\n",
    "    \"\"\"Define Jaccard Similarity function for two sets\"\"\"\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dictionaries of city number city code and city center coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary from city number to city code\n",
    "def dict_cityN_cityC():\n",
    "    dataframe= pd.read_csv('Clean_dataset/clean_data_comprehensive_final.csv')\n",
    "    dict_cityN_to_cityC={}\n",
    "    list_pair= []\n",
    "    for i in range(len(dataframe)):\n",
    "        list_pair.append((dataframe.loc[i, 'city_n_origin'], dataframe.loc[i, 'Codigo_Municipio_de_Origem']))\n",
    "        list_pair.append((dataframe.loc[i, 'city_n_destination'], dataframe.loc[i, 'Codigo_Municipio_de_Destino']))\n",
    "    \n",
    "    list_pair=list(set(list_pair))\n",
    "    for i in range(len(list_pair)):\n",
    "        dict_cityN_to_cityC[list_pair[i][0]]= list_pair[i][1]\n",
    "    return  dict_cityN_to_cityC\n",
    "\n",
    "def dict_cityC_cityN():\n",
    "    dataframe= pd.read_csv('Clean_dataset/clean_data_comprehensive_final.csv')\n",
    "    dict_cityN_to_cityC={}\n",
    "    list_pair= []\n",
    "    for i in range(len(dataframe)):\n",
    "        list_pair.append((dataframe.loc[i, 'city_n_origin'], dataframe.loc[i, 'Codigo_Municipio_de_Origem']))\n",
    "        list_pair.append((dataframe.loc[i, 'city_n_destination'], dataframe.loc[i, 'Codigo_Municipio_de_Destino']))\n",
    "    \n",
    "    list_pair=list(set(list_pair))\n",
    "    for i in range(len(list_pair)):\n",
    "        dict_cityN_to_cityC[list_pair[i][1]]= list_pair[i][0]\n",
    "    return  dict_cityN_to_cityC\n",
    "\n",
    "def dict_cityN_cityCoo():\n",
    "    mg_ma=  gpd.read_file('mg_gps_map_data/mg_city/31MUE250GC_SIR.shp')\n",
    "    mg_ma['center_co']= mg_ma['geometry'].centroid\n",
    "    dict_cityc_cityn= dict_cityC_cityN()\n",
    "    dict_cityN_to_cityCoo={}\n",
    "    for i in range(len(mg_ma)):\n",
    "        cn= dict_cityc_cityn[int(mg_ma.loc[i, 'CD_GEOCMU'])] \n",
    "        dict_cityN_to_cityCoo[cn] = mg_ma.loc[i, 'center_co']\n",
    "    \n",
    "    return dict_cityN_to_cityCoo \n",
    "        \n",
    "############################################### Transformed Coordination #################################################\n",
    "def dict_cityN_cityCoo_T():\n",
    "    mg_ma=  gpd.read_file('mg_gps_map_data/mg_map_transform_87_118/mg_map_transform_87_118.shp')\n",
    "    mg_ma['center_co']= mg_ma['geometry'].centroid\n",
    "    dict_cityc_cityn= dict_cityC_cityN()\n",
    "    dict_cityN_to_cityCoo={}\n",
    "    for i in range(len(mg_ma)):\n",
    "        cn= dict_cityc_cityn[int(mg_ma.loc[i, 'CD_GEOCMU'])] \n",
    "        dict_cityN_to_cityCoo[cn] = mg_ma.loc[i, 'center_co']\n",
    "    \n",
    "    return dict_cityN_to_cityCoo \n",
    "    \n",
    "  \n",
    "    \n",
    "############################################### dictionary of city_number to miroregion name############################\n",
    "def dict_cityNu_microNa(data_part):\n",
    "    d1= dict(zip(data_part.city_n_origin, data_part.oigin_microregion_name))\n",
    "    d2= dict(zip(data_part.city_n_destination, data_part.destination_microregion_name))\n",
    "    d = {**d1, **d2}\n",
    "    #d= z = d1 | d2\n",
    "    #d= dict(d1.items() + d2.items())\n",
    "    #remove duplicated items\n",
    "    result = {}\n",
    "\n",
    "    for key,value in d.items():\n",
    "        if key not in result.keys():\n",
    "            result[key] = value\n",
    "            \n",
    "    return result\n",
    "    \n",
    "    \n",
    "############################################### dictionary of city_number to miroregion name############################\n",
    "def dict_cityNu_mesoNa(data_part):\n",
    "    d1= dict(zip(data_part.city_n_origin, data_part.oigin_mesoregion_name))\n",
    "    d2= dict(zip(data_part.city_n_destination, data_part.destination_mesoregion_name))\n",
    "    d = {**d1, **d2}\n",
    "    #d= z = d1 | d2\n",
    "    #d= dict(d1.items() + d2.items())\n",
    "    #remove duplicated items\n",
    "    result = {}\n",
    "\n",
    "    for key,value in d.items():\n",
    "        if key not in result.keys():\n",
    "            result[key] = value\n",
    "            \n",
    "    return result   \n",
    "\n",
    "def dict_micNa_micCoo():\n",
    "    \n",
    "    mg_mcrrgn_map=gpd.read_file('mg_gps_map_data/mcrrgn_mg/31MI2500G.shp')\n",
    "    a = [mg_mcrrgn_map[\"geocodigo\"], mg_mcrrgn_map[\"nome\"],mg_mcrrgn_map['geometry']]\n",
    "    headers = [\"microregion_code\", \"name\",'geometry']\n",
    "    mg_mcrrgn_map = pd.concat(a, axis=1, keys=headers)\n",
    "    mg_mcrrgn_map=gpd.GeoDataFrame(mg_mcrrgn_map)\n",
    "    #assign number to microregion\n",
    "    mg_mcrrgn_map['microregion_number']=mg_mcrrgn_map.index \n",
    "    mg_mcrrgn_map['center_co_mic']=mg_mcrrgn_map['geometry'].centroid\n",
    "    dict_micna_miccoo={}\n",
    "    for i in range(len(mg_mcrrgn_map)):\n",
    "        dict_micna_miccoo[mg_mcrrgn_map.loc[i, 'name']] = mg_mcrrgn_map.loc[i, 'center_co_mic']\n",
    "    \n",
    "    return dict_micna_miccoo\n",
    "\n",
    "def dict_micNu_micNa():\n",
    "    \n",
    "    mg_mcrrgn_map=gpd.read_file('mg_gps_map_data/mcrrgn_mg/31MI2500G.shp')\n",
    "    a = [mg_mcrrgn_map[\"geocodigo\"], mg_mcrrgn_map[\"nome\"],mg_mcrrgn_map['geometry']]\n",
    "    headers = [\"microregion_code\", \"name\",'geometry']\n",
    "    mg_mcrrgn_map = pd.concat(a, axis=1, keys=headers)\n",
    "    mg_mcrrgn_map=gpd.GeoDataFrame(mg_mcrrgn_map)\n",
    "    #assign number to microregion\n",
    "    mg_mcrrgn_map['microregion_number']=mg_mcrrgn_map.index \n",
    "    mg_mcrrgn_map['center_co_mic']=mg_mcrrgn_map['geometry'].centroid\n",
    "    dict_micna_miccoo={}\n",
    "    for i in range(len(mg_mcrrgn_map)):\n",
    "        dict_micna_miccoo[mg_mcrrgn_map.loc[i, 'microregion_number']] = mg_mcrrgn_map.loc[i, 'name']\n",
    "    \n",
    "    return dict_micna_miccoo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add values realated to the cities to gps dataset for ploting\n",
    "\n",
    "def map_city_value(list_cityN_value):\n",
    "#list_cityN_value: a list of pars. for each pair the first element is city number and second element \n",
    "#is value related to that city\n",
    "    #dictionary of find the city code based on city number\n",
    "    #dictionary[city_number]= city_code\n",
    "    dict_cityN_to_cityC = np.load('mg_gps_map_data/dict_cityNunmber_to_cityCode.npy',allow_pickle='TRUE').item()\n",
    "    \n",
    "    #the dataframe of polygon data of cities\n",
    "    mg_ma=gpd.read_file('mg_gps_map_data/mg_city/31MUE250GC_SIR.shp')\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    for i in list_cityN_value:\n",
    "        cc= dict_cityN_to_cityC[i[0]]\n",
    "        for j in range(len(mg_ma)):\n",
    "            if int(mg_ma.loc[j, 'CD_GEOCMU' ])== cc:\n",
    "                mg_ma.loc[j, 'value']= i[1]\n",
    "    return mg_ma\n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portrait Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions are needed\n",
    "\n",
    "def get_unique_path_lengths(graph, paths=None):\n",
    "    if paths is None:\n",
    "        paths = list(nx.all_pairs_dijkstra_path_length(graph))\n",
    "\n",
    "    unique_path_lengths = set()\n",
    "    for starting_node, dist_dict in paths:\n",
    "        unique_path_lengths |= set(dist_dict.values())\n",
    "    unique_path_lengths = sorted(list(unique_path_lengths))\n",
    "    return unique_path_lengths\n",
    "\n",
    "\n",
    "def get_portrait(graph):\n",
    "#     giant = max(nx.connected_component_subgraphs(graph), key=len)\n",
    "    diameter = 500 #nx.diameter(giant.to_undirected())\n",
    "    N = graph.number_of_nodes()\n",
    "    # B indices are 0...dia x 0...N-1:\n",
    "    B = np.zeros((diameter + 1, N))\n",
    "\n",
    "    max_path = 1\n",
    "    adj = graph.adj\n",
    "    for starting_node in graph.nodes():\n",
    "        nodes_visited = {starting_node: 0}\n",
    "        search_queue = [starting_node]\n",
    "        d = 1\n",
    "        while search_queue:\n",
    "            next_depth = []\n",
    "            extend = next_depth.extend\n",
    "            for n in search_queue:\n",
    "                l = [i for i in adj[n] if i not in nodes_visited]\n",
    "                extend(l)\n",
    "                for j in l:\n",
    "                    nodes_visited[j] = d\n",
    "            search_queue = next_depth\n",
    "            d += 1\n",
    "\n",
    "        node_distances = nodes_visited.values()\n",
    "        max_node_distances = max(node_distances)\n",
    "\n",
    "        curr_max_path = max_node_distances\n",
    "        if curr_max_path > max_path:\n",
    "            max_path = curr_max_path\n",
    "\n",
    "        # build individual distribution:\n",
    "        dict_distribution = dict.fromkeys(node_distances, 0)\n",
    "        for d in node_distances:\n",
    "            dict_distribution[d] += 1\n",
    "        # add individual distribution to matrix:\n",
    "        for shell, count in dict_distribution.items():\n",
    "            B[shell][count] += 1\n",
    "\n",
    "        # HACK: count starting nodes that have zero nodes in farther shells\n",
    "        max_shell = diameter\n",
    "        while max_shell > max_node_distances:\n",
    "            B[max_shell][0] += 1\n",
    "            max_shell -= 1\n",
    "\n",
    "    return B[:max_path + 1, :]\n",
    "\n",
    "\n",
    "def pad_portraits_to_same_size(b1, b2):\n",
    "    ns, ms = b1.shape\n",
    "    nl, ml = b2.shape\n",
    "\n",
    "    # Bmats have N columns, find last *occupied* column and trim both down:\n",
    "    last_col_1 = max(np.nonzero(b1)[1])\n",
    "    last_col_2 = max(np.nonzero(b2)[1])\n",
    "    last_col = max(last_col_1, last_col_2)\n",
    "    b1 = b1[:, :last_col + 1]\n",
    "    b2 = b2[:, :last_col + 1]\n",
    "\n",
    "    big_b1 = np.zeros((max(ns, nl), last_col + 1))\n",
    "    big_b2 = np.zeros((max(ns, nl), last_col + 1))\n",
    "\n",
    "    big_b1[:b1.shape[0], :b1.shape[1]] = b1\n",
    "    big_b2[:b2.shape[0], :b2.shape[1]] = b2\n",
    "\n",
    "    return big_b1, big_b2\n",
    "\n",
    "\n",
    "def zeros(shape, tCode=None):\n",
    "    try:\n",
    "        return np.zeros(shape, dtype=tCode)\n",
    "    except TypeError:\n",
    "        return np.zeros(shape, typecode='fd')  # hardwired to float\n",
    "\n",
    "\n",
    "def graph_or_portrait(x):\n",
    "    if isinstance(x, (nx.Graph, nx.DiGraph)):\n",
    "        return get_portrait(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def element_wise_log(mat):\n",
    "    new_mat = zeros(mat.shape, tCode=float)\n",
    "    i = 0\n",
    "    for row in mat:\n",
    "        j = 0\n",
    "        for e in row:\n",
    "            if e != 0:\n",
    "                new_mat[i, j] = log(e + 1)\n",
    "            else:\n",
    "                new_mat[i, j] = 0\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return new_mat\n",
    "\n",
    "\n",
    "def create_matrix_plot(o_mat, title, **kwargs):\n",
    "    kwargs['interpolation'] = 'nearest'\n",
    "    origin = kwargs.get('origin', 1)\n",
    "    kwargs['origin'] = 'lower'\n",
    "    showColorBar = kwargs.get('showColorBar', False)\n",
    "    if \"showColorBar\" in kwargs: kwargs.pop(\"showColorBar\")\n",
    "    logColors = kwargs.get('logColors', False)\n",
    "    if \"logColors\" in kwargs: kwargs.pop(\"logColors\")\n",
    "    ifShow = kwargs.get('show', False)\n",
    "    if \"show\" in kwargs: kwargs.pop(\"show\")\n",
    "    fileName = kwargs.get('fileName', None)\n",
    "    if \"fileName\" in kwargs: kwargs.pop(\"fileName\")\n",
    "\n",
    "    mat = o_mat.copy()  # don't modify original matrix\n",
    "    if logColors: mat = element_wise_log(mat)\n",
    "\n",
    "    cmap = plt.cm.seismic\n",
    "    cmap.set_under(color='white')\n",
    "\n",
    "    if \"vmax\" not in kwargs:\n",
    "        kwargs['vmax'] = float(mat[origin:, origin:].max())\n",
    "        kwargs['vmin'] = float(mat[origin:, origin:].min() + sys.float_info.epsilon)\n",
    "        kwargs['cmap'] = cmap\n",
    "\n",
    "    ax = plt.axes()  # [.05,.05,.9,.9])\n",
    "    ax.xaxis.tick_top()\n",
    "    h = plt.imshow(mat, **kwargs)\n",
    "    plt.title(title)\n",
    "    plt.axis('tight')\n",
    "    # ax.set_xlim((origin, 10))\n",
    "    ax.set_ylim((mat.shape[0], origin))\n",
    "\n",
    "    if showColorBar: plt.colorbar()\n",
    "\n",
    "    if fileName is not None:\n",
    "        plt.savefig(fileName)\n",
    "        \n",
    "    if ifShow:    \n",
    "        plt.show()\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "def portrait_divergence(g, h):\n",
    "    bg = graph_or_portrait(g)\n",
    "    bh = graph_or_portrait(h)\n",
    "    bg, bh = pad_portraits_to_same_size(bg, bh)\n",
    "\n",
    "    l, k = bg.shape\n",
    "    v = np.tile(np.arange(k), (l, 1))\n",
    "\n",
    "    xg = bg * v / (bg * v).sum()\n",
    "    xh = bh * v / (bh * v).sum()\n",
    "\n",
    "    # flatten distribution matrices as arrays:\n",
    "    p = xg.ravel()\n",
    "    q = xh.ravel()\n",
    "\n",
    "    # lastly, get JSD:\n",
    "    m = 0.5 * (p + q)\n",
    "    kld_pm = entropy(p, m, base=2)\n",
    "    kld_qm = entropy(q, m, base=2)\n",
    "    jsd_pq = 0.5 * (kld_pm + kld_qm)\n",
    "\n",
    "    return jsd_pq, bg, bh\n",
    "\n",
    "\n",
    "def weighted_portrait(g, paths=None, bin_edges=None):\n",
    "    from collections import Counter\n",
    "    if paths is None:\n",
    "        paths = list(nx.all_pairs_dijkstra_path_length(g))\n",
    "\n",
    "    if bin_edges is None:\n",
    "        unique_path_lengths = get_unique_path_lengths(g, paths=paths)\n",
    "        sampled_path_lengths = np.percentile(unique_path_lengths, np.arange(0, 101, 1))\n",
    "    else:\n",
    "        sampled_path_lengths = bin_edges\n",
    "    upl = np.array(sampled_path_lengths)\n",
    "\n",
    "    l_s_v = []\n",
    "    for i, (s, dist_dict) in enumerate(paths):\n",
    "        distances = np.array(list(dist_dict.values()))\n",
    "        s_v, e = np.histogram(distances, bins=upl)\n",
    "        l_s_v.append(s_v)\n",
    "    m = np.array(l_s_v)\n",
    "\n",
    "    b = np.zeros((len(upl) - 1, g.number_of_nodes() + 1))\n",
    "    for i in range(len(upl) - 1):\n",
    "        col = m[:, i]  # ith col = numbers of nodes at d_i <= distance < d_i+1\n",
    "        for n, c in Counter(col).items():\n",
    "            b[i, n] += c\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "def portrait_divergence_weighted(g, h, bins=None, bin_edges=None):\n",
    "    from collections import Counter\n",
    "    from scipy.stats import entropy\n",
    "    # get joint binning:\n",
    "    paths_g = list(nx.all_pairs_dijkstra_path_length(g))\n",
    "    paths_h = list(nx.all_pairs_dijkstra_path_length(h))\n",
    "\n",
    "    # get bin_edges in common for G and H:\n",
    "    if bin_edges is None:\n",
    "        if bins is None:\n",
    "            bins = 1\n",
    "        upl_g = set(get_unique_path_lengths(g, paths=paths_g))\n",
    "        upl_h = set(get_unique_path_lengths(h, paths=paths_h))\n",
    "        unique_path_lengths = sorted(list(upl_g | upl_h))\n",
    "        bin_edges = np.percentile(unique_path_lengths, np.arange(0, 101, bins))\n",
    "\n",
    "    # get weighted portraits:\n",
    "    bg = weighted_portrait(g, paths=paths_g, bin_edges=bin_edges)\n",
    "    bh = weighted_portrait(h, paths=paths_h, bin_edges=bin_edges)\n",
    "\n",
    "    return portrait_divergence(bg, bh)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
