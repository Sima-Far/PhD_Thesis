{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Function_network.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform the geo shp file to the range we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_transform(h, w):#\n",
    "    global map_height, map_width, count, centers_count, min_map_x, max_map_x, min_map_y, max_map_y\n",
    "    \n",
    "    #mg_map_transform= gpd.read_file('mg_gps_map_data/mg_map_transform/mg_map_transform.shp')\n",
    "    mg_map=gpd.read_file('mg_gps_map_data/mg_city/31MUE250GC_SIR.shp')\n",
    "    mg_map['center_co']=mg_map['geometry'].centroid\n",
    "\n",
    "    #mg_map_transform['center_co']=mg_map_transform['geometry'].centroid\n",
    "    \n",
    "    \n",
    "    min_map_x=min(list(mg_map['geometry'][547].exterior.coords.xy)[0]) \n",
    "    max_map_x=max(list(mg_map['geometry'][704].exterior.coords.xy)[0]) \n",
    "    min_map_y=min(list(mg_map['geometry'][171].exterior.coords.xy)[1]) \n",
    "    max_map_y=max(list(mg_map['geometry'][164].exterior.coords.xy)[1])\n",
    "    map_height=max_map_y-min_map_y\n",
    "    map_width=max_map_x-min_map_x\n",
    "    #w, h = int(11.189252920000001*100) , int(8.672043336000002*100) \n",
    "    \n",
    "    count = 12\n",
    "    #change number of cities/mics here\n",
    "    centers_count = 853 \n",
    "    \n",
    "    \n",
    "    #funcrtion to transform\n",
    "def map_to_numpy(points):\n",
    "    initialize_transform(h , w)\n",
    "    res = points.copy()\n",
    "    x_map, y_map = res[:,0], res[:,1]\n",
    "    x_map = x_map - min_map_x\n",
    "    y_map = y_map - min_map_y\n",
    "    normalized_x = x_map / map_width # from 0 to 1 west to east\n",
    "    normalized_y = y_map / map_height # from 0 to 1 south to north\n",
    "    y_numpy = normalized_x * w\n",
    "    x_numpy = (1 - normalized_y) * h\n",
    "    return np.dstack([x_numpy, y_numpy])[0]#.astype(int)\n",
    "# اگه اسنجا ازتایپ اینتیجر باشه تو جابه جایی گوچیک به 86 و 111 خیلی نقشه خراب میشه مرزهاش و فیلد درست درنمیاد"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_sum_mean_dx_dy_outgoing_city(city_number,data_part,dict_cityn_citycoo, method):\n",
    "    #data_part: the data complete that includes city numbers\n",
    "    #method: mean or sum\n",
    "    data_one_city_code=data_part[data_part['city_n_origin']== city_number]\n",
    "    data_one_city_code= data_one_city_code.reset_index(drop=True)\n",
    "    data_one_city_feature_list=[]\n",
    "    #find city center coordination of origin from city number\n",
    "    origin_center= dict_cityn_citycoo[city_number] \n",
    "    \n",
    "    #loop on different destinations to find vector for each trade\n",
    "    for s in range(len(data_one_city_code)):\n",
    "        #city number of destination\n",
    "        destination_number= data_one_city_code.loc[s,'city_n_destination']\n",
    "        #city center coordination of destination using dictionary from dict_cityN_cityCoo function\n",
    "        geo_destination= dict_cityn_citycoo[destination_number]\n",
    "    \n",
    "        \n",
    "        d_y=geo_destination.y-origin_center.y\n",
    "        d_x=geo_destination.x-origin_center.x\n",
    "        data_one_city_feature_list.append((d_x,d_y))\n",
    "\n",
    "    dx_sum=sum(first for first, second in data_one_city_feature_list)\n",
    "    dy_sum=sum(second for first, second in data_one_city_feature_list)\n",
    "    \n",
    "    if len(data_one_city_feature_list)!=0: \n",
    "        dx_mean=sum(first for first, second in data_one_city_feature_list)/len(data_one_city_feature_list)\n",
    "        dy_mean=sum(second for first, second in data_one_city_feature_list)/len(data_one_city_feature_list)\n",
    "    else:\n",
    "        dx_mean=0\n",
    "        dy_mean=0\n",
    "        \n",
    "    if method=='sum':\n",
    "        return (dx_sum,dy_sum)\n",
    "    if method=='mean':\n",
    "        return (dx_mean,dy_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate vector microregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_sum_mean_dx_dy_outgoing_mic(mic_name,data_part,dict_minname_miccoo, method):\n",
    "\n",
    "    #data_part: the data complete that includes city numbers\n",
    "    #method: mean or sum\n",
    "    data_one_city_code=data_part[data_part['oigin_microregion_name']== mic_name]\n",
    "    data_one_city_code= data_one_city_code.reset_index(drop=True)\n",
    "    data_one_city_feature_list=[]\n",
    "    #find city center coordination of origin from city number\n",
    "    origin_center= dict_minname_miccoo[mic_name] \n",
    "    \n",
    "    #loop on different destinations to find vector for each trade\n",
    "    for s in range(len(data_one_city_code)):\n",
    "        #city number of destination\n",
    "        destination_number= data_one_city_code.loc[s,'destination_microregion_name']\n",
    "        #city center coordination of destination using dictionary from dict_cityN_cityCoo function\n",
    "        geo_destination= dict_minname_miccoo[destination_number]\n",
    "    \n",
    "        \n",
    "        d_y=geo_destination.y-origin_center.y\n",
    "        d_x=geo_destination.x-origin_center.x\n",
    "        data_one_city_feature_list.append((d_x,d_y))\n",
    "\n",
    "    dx_sum=sum(first for first, second in data_one_city_feature_list)\n",
    "    dy_sum=sum(second for first, second in data_one_city_feature_list)\n",
    "    \n",
    "    if len(data_one_city_feature_list)!=0: \n",
    "        dx_mean=sum(first for first, second in data_one_city_feature_list)/len(data_one_city_feature_list)\n",
    "        dy_mean=sum(second for first, second in data_one_city_feature_list)/len(data_one_city_feature_list)\n",
    "    else:\n",
    "        dx_mean=0\n",
    "        dy_mean=0\n",
    "        \n",
    "    if method=='sum':\n",
    "        return (dx_sum,dy_sum)\n",
    "    if method=='mean':\n",
    "        return (dx_mean,dy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_sum_mean_dx_dy_outgoing_city_transform(city_number,data_part,dict_cityn_citycoo_t, method):\n",
    "    #data_part: the data complete that includes city numbers\n",
    "    #method: mean or sum\n",
    "    data_one_city_code=data_part[data_part['city_n_origin']== city_number]\n",
    "    data_one_city_code= data_one_city_code.reset_index(drop=True)\n",
    "    data_one_city_feature_list=[]\n",
    "\n",
    "    #find city center coordination of origin from city number\n",
    "    origin_center= dict_cityn_citycoo_t[city_number] \n",
    "    \n",
    "    #loop on different destinations to find vector for each trade\n",
    "    for s in range(len(data_one_city_code)):\n",
    "        #city number of destination\n",
    "        destination_number= data_one_city_code.loc[s,'city_n_destination']\n",
    "        #city center coordination of destination using dictionary from dict_cityN_cityCoo function\n",
    "        geo_destination= dict_cityn_citycoo_t[destination_number]\n",
    "    \n",
    "        \n",
    "        d_y=geo_destination.y-origin_center.y\n",
    "        d_x=geo_destination.x-origin_center.x\n",
    "        data_one_city_feature_list.append((d_x,d_y))\n",
    "\n",
    "    dx_sum=sum(first for first, second in data_one_city_feature_list)\n",
    "    dy_sum=sum(second for first, second in data_one_city_feature_list)\n",
    "    \n",
    "    if len(data_one_city_feature_list)!=0: \n",
    "        dx_mean=sum(first for first, second in data_one_city_feature_list)/len(data_one_city_feature_list)\n",
    "        dy_mean=sum(second for first, second in data_one_city_feature_list)/len(data_one_city_feature_list)\n",
    "    else:\n",
    "        dx_mean=0\n",
    "        dy_mean=0\n",
    "        \n",
    "    if method=='sum':\n",
    "        return (dx_sum,dy_sum)\n",
    "    if method=='mean':\n",
    "        return (dx_mean,dy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_sim_mc(vector_mc):\n",
    "    n_mc= vector_mc.shape[0]\n",
    "    wn= vector_mc.shape[2]\n",
    "    vector_mc_cosine_sim=np.full((n_mc,wn-1),0,float) #**********************************************************None\n",
    "    for i in range(n_mc):\n",
    "        for w in range(wn-1):\n",
    "            v1_x=vector_mc[i,0,w]\n",
    "            v1_y=vector_mc[i,1,w]\n",
    "            v2_x=vector_mc[i,0,w+1]\n",
    "            v2_y=vector_mc[i,1,w+1]\n",
    "            f_i=math.sqrt(v1_x**2+v1_y**2)\n",
    "            f_j=math.sqrt(v2_x**2+v2_y**2)\n",
    "            dot_p=(v1_x*v2_x)+(v1_y*v2_y)\n",
    "            if f_i*f_j!=0:\n",
    "                #for microregions all micregion have vector\n",
    "                #for cities: there are some cities that their vector is xero for some month because they do not have tarde\n",
    "                #or the mean\n",
    "                co_value=dot_p/(f_i*f_j)\n",
    "                vector_mc_cosine_sim[i,w]= co_value\n",
    "            \n",
    "    return vector_mc_cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dot_p_sim(vector_all):\n",
    "    nu_p= np.shape(vector_all)\n",
    "    vector_cosine_sim=np.full((nu_p[0],nu_p[2]-1),None,float) #nu_p[2]-1 is 11 consecutive months\n",
    "    for i in range(nu_p[0]):\n",
    "        for w in range(nu_p[2]-1):\n",
    "            v1_x= vector_all[i,0,w]\n",
    "            v1_y= vector_all[i,1,w]\n",
    "            v2_x= vector_all[i,0,w+1]\n",
    "            v2_y= vector_all[i,1,w+1]\n",
    "            dot_p=(v1_x*v2_x)+(v1_y*v2_y)\n",
    "            vector_cosine_sim[i,w]= dot_p\n",
    "            \n",
    "    return vector_cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_sim(vector_all):\n",
    "    nu_p= np.shape(vector_all)\n",
    "    vector_cosine_sim=np.full((nu_p[0],nu_p[2]-1),None,float) #nu_p[2]-1 is 11 consecutive months\n",
    "    for i in range(nu_p[0]):\n",
    "        for w in range(nu_p[2]-1):\n",
    "            v1_x=vector_all[i,0,w]\n",
    "            v1_y= vector_all[i,1,w]\n",
    "            v2_x= vector_all[i,0,w+1]\n",
    "            v2_y=vector_all [i,1,w+1]\n",
    "            f_i=math.sqrt(v1_x**2+v1_y**2)\n",
    "            f_j=math.sqrt(v2_x**2+v2_y**2)\n",
    "            dot_p=(v1_x*v2_x)+(v1_y*v2_y)\n",
    "            if f_i*f_j!=0:\n",
    "                #for microregions all micregion have vector\n",
    "                #for cities: there are some cities that their vector is xero for some month because they do not have tarde\n",
    "                #or the mean\n",
    "                co_value=dot_p/(f_i*f_j)\n",
    "                vector_cosine_sim[i,w]= co_value\n",
    "            \n",
    "    return vector_cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mine (ermia code after I changed it)(numpy version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBF function\n",
    "def init_parameters(dc):\n",
    "    min_map_x=min(list(dc['geometry'][547].exterior.coords.xy)[0]) \n",
    "    max_map_x=max(list(dc['geometry'][704].exterior.coords.xy)[0]) \n",
    "    min_map_y=min(list(dc['geometry'][171].exterior.coords.xy)[1]) \n",
    "    max_map_y=max(list(dc['geometry'][164].exterior.coords.xy)[1]) \n",
    "    \n",
    "    global h, w, x, y\n",
    "    h=int((max_map_y-min_map_y)*10)\n",
    "    w=int((max_map_x-min_map_x)*10)\n",
    "    width = np.linspace(min_map_x, max_map_x, w)   #range -a to +a\n",
    "    height = np.linspace(min_map_y, max_map_y,h)\n",
    "    x, y = np.meshgrid(width, height)\n",
    "\n",
    "    #قبل اینکه کل دیتا ست و مرکز شهرها رو ترنسفورم کنیم ایکس نقشه از وای نقشه بزرگتر بود ولی بعد از ترنسفورم  وای نقشه از ایکس بزرگتر است \n",
    "\n",
    "def point_inside_map():\n",
    "    mg_boundary= gpd.read_file('mg_gps_map_data/MG_total_boundry/th878nx5786.shp') \n",
    "    polygon_boundry= mg_boundary['geometry'][0]\n",
    "    x_y_in=[]\n",
    "\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        for j in range(np.shape(x)[1]):\n",
    "            point=Point(x[i,j],y[i,j])\n",
    "            if polygon_boundry.contains(point) is True:\n",
    "                x_y_in.append((x[i,j],y[i,j]))\n",
    "    return x_y_in\n",
    "\n",
    "    \n",
    "    \n",
    "def rbf_interpolate(x_known, y_known, vecs):  \n",
    "    point_inside= point_inside_map()\n",
    "    rbf1 = Rbf(x_known, y_known, vecs[:, 0], epsilon=3, function='gaussian')\n",
    "    rbf2 = Rbf(x_known, y_known, vecs[:, 1], epsilon=3, function='gaussian')\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        for j in range(np.shape(x)[1]):\n",
    "            if (x[i,j],y[i,j]) in point_inside:\n",
    "                u[i,j] = rbf1(x[i,j],y[i,j])\n",
    "                v[i,j] = rbf2(x[i,j],y[i,j])\n",
    "            else:\n",
    "                u[i,j] = 0\n",
    "                v[i,j] = 0\n",
    "    return np.dstack([u, v])        \n",
    "\n",
    "\n",
    "\n",
    "def interpolate_field(centers, vectors):\n",
    "    x_known = centers[:, 0]\n",
    "    y_known = centers[:, 1]\n",
    "    field = rbf_interpolate(x_known, y_known, vectors)\n",
    "    return field\n",
    "\n",
    "\n",
    "#find field for all 365 day/or 52-week/or 12-month\n",
    "def find_vector_fields_rbf(city_coords, vecs):\n",
    "    fields = np.zeros((count, w, h, 2))\n",
    "    for i in range(count):\n",
    "        fields[i] = interpolate_field(city_coords, vecs[:,:, i]) \n",
    "        print('field ' + str(i + 1) + ' found')\n",
    "    return fields\n",
    "\n",
    "def find_and_save_fields_rbf(city_center, vectors_name, save_name):\n",
    "    #city_coords, vecs = init_vecs(center_names, vectors_name)\n",
    "    city_coords= np.loadtxt(city_center)\n",
    "    vecs= np.loadtxt(vectors_name)\n",
    "    vecs = vecs.reshape((centers_count, 2, count))\n",
    "    fields= find_vector_fields_rbf(city_coords, vecs)\n",
    "    \n",
    "    #save the field for all days/weeks/months\n",
    "    saved = fields.reshape(count * 2 * h * w)\n",
    "    np.savetxt(save_name, saved)\n",
    "    return fields\n",
    "\n",
    "    \n",
    "def load_fields_rbf(name):\n",
    "    fields = np.loadtxt(name)\n",
    "    fields = fields.reshape((count, w, h, 2))\n",
    "    return fields\n",
    "\n",
    "\n",
    "def magnitude(vector):\n",
    "    r=vector*vector\n",
    "    r=r.sum(axis=2)\n",
    "    r=np.sqrt(r)\n",
    "    return r\n",
    "\n",
    "\n",
    "def normalize_f(vector):\n",
    "    r=magnitude(vector)\n",
    "    rn=r.repeat(2).reshape((vector.shape[0],vector.shape[1],2))\n",
    "    rn=vector/rn\n",
    "    d=1\n",
    "    rn=rn*d\n",
    "    return rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude(vector):\n",
    "    r=vector*vector\n",
    "    r=r.sum(axis=2)\n",
    "    r=np.sqrt(r)\n",
    "    return r\n",
    "\n",
    "\n",
    "def normalize_f(vector):\n",
    "    r=magnitude(vector)\n",
    "    rn=r.repeat(2).reshape((vector.shape[0],vector.shape[1],2))\n",
    "    rn=vector/rn\n",
    "    d=1\n",
    "    rn=rn*d\n",
    "    return rn\n",
    "\n",
    "def draw_arrow_on_ax(i,j,u,v,colorval,ax):\n",
    "    v1_x=i\n",
    "    v1_y=j\n",
    "    ax.plot(i,j,linestyle='',color='black') \n",
    "    ax.arrow(v1_x,v1_y,u,v,head_width=2,\\\n",
    "        head_length=2,width=1,linewidth=1, length_includes_head=False,facecolor=colorval,edgecolor='black')\n",
    "    \n",
    "    \n",
    "def plot_field(data_vector):#data_vector should be a (h,w,2) matrix. ha and w are the \n",
    "                        #coordination of the vector with 2 elements\n",
    "    r = magnitude(data_vector)\n",
    "    rn = normalize_f(data_vector)\n",
    "    max_size= r.max()\n",
    "    #plot the maps\n",
    "    cmap = plt.cm.autumn\n",
    "    fig, ax = plt.subplots(1,1,figsize=[10,10])\n",
    "    cNorm  = colors.Normalize(vmin=0, \\\n",
    "                                  vmax=max_size)\n",
    "    scalarMap = cmx.ScalarMappable(norm=cNorm,cmap=cmap)\n",
    "    #print(data_vector[220,255])\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            if data_vector[i,j,0]==0 and data_vector[i,j,1]==0:\n",
    "                pass                \n",
    "            else:\n",
    "                colorval = scalarMap.to_rgba(r[i,j])\n",
    "                draw_arrow_on_ax(i,j,rn[i,j,0], rn[i,j,1],colorval,ax)\n",
    "                \n",
    "   \n",
    "    return ax\n",
    "\n",
    "\n",
    "def find_critical_points(field, threshold):\n",
    "    magnitude = np.sqrt(field[:, :, 0]**2 + field[:, :, 1]**2)\n",
    "    indices = np.where(magnitude[1:-1, 1:-1] < threshold)\n",
    "    return np.dstack([indices[0], indices[1]])[0] + [1, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mine (ermia code after I changed it)(Pandas version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBF function\n",
    "def init_parameters(dc):\n",
    "    min_map_x=min(list(dc['geometry'][547].exterior.coords.xy)[0]) \n",
    "    max_map_x=max(list(dc['geometry'][704].exterior.coords.xy)[0]) \n",
    "    min_map_y=min(list(dc['geometry'][171].exterior.coords.xy)[1]) \n",
    "    max_map_y=max(list(dc['geometry'][164].exterior.coords.xy)[1]) \n",
    "    \n",
    "    global h, w, x, y\n",
    "    h=int((max_map_y-min_map_y)*10)\n",
    "    w=int((max_map_x-min_map_x)*10)\n",
    "    width = np.linspace(min_map_x, max_map_x, w)   #range -a to +a\n",
    "    height = np.linspace(min_map_y, max_map_y,h)\n",
    "    x, y = np.meshgrid(width, height)\n",
    "\n",
    "    #قبل اینکه کل دیتا ست و مرکز شهرها رو ترنسفورم کنیم ایکس نقشه از وای نقشه بزرگتر بود ولی بعد از ترنسفورم  وای نقشه از ایکس بزرگتر است \n",
    "\n",
    "def point_inside_map(x,y):\n",
    "    mg_boundary= gpd.read_file('mg_gps_map_data/MG_total_boundry/th878nx5786.shp') \n",
    "    polygon_boundry=mg_boundary['geometry'][0]\n",
    "    x_y_in=[]\n",
    "\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        for j in range(np.shape(x)[1]):\n",
    "            point=Point(x[i,j],y[i,j])\n",
    "            if polygon_boundry.contains(point) is True:\n",
    "                x_y_in.append((x[i,j],y[i,j]))\n",
    "    return x_y_in\n",
    "\n",
    "def rbf_interpolate_pandas(x_known, y_known, vecs):  \n",
    "    l1=[]\n",
    "    point_inside= point_inside_map()\n",
    "    rbf1 = Rbf(x_known, y_known, vecs[:, 0], epsilon=3, function='gaussian')\n",
    "    rbf2 = Rbf(x_known, y_known, vecs[:, 1], epsilon=3, function='gaussian')\n",
    "    for el in point_inside:\n",
    "        l1.append(((el[0],el[1]),(rbf1(el[0],el[1]),rbf2(el[0],el[1]))))\n",
    "    return l1\n",
    "\n",
    "\n",
    "def interpolate_field_pandas(centers, vectors):\n",
    "    x_known = centers[:, 0]\n",
    "    y_known = centers[:, 1]\n",
    "    point_vector_list = rbf_interpolate_pandas(x_known, y_known, vectors)\n",
    "    return [e[1] for e in point_vector_list]\n",
    "\n",
    "def find_vector_fields_rbf_pandas(city_coords, vecs):\n",
    "    fields = pd.DataFrame() \n",
    "    point_inside= point_inside_map()\n",
    "    fields['point']= point_inside\n",
    "    for i in range(count):\n",
    "        fields['vector_field_{}'.format(i)] = interpolate_field_pandas(city_coords, vecs[:,:, i]) \n",
    "        print('field ' + str(i + 1) + ' found')\n",
    "    return fields\n",
    "\n",
    "\n",
    "\n",
    "def find_and_save_fields_rbf_pandas(city_center, vectors_name, save_name):\n",
    "    #city_coords, vecs = init_vecs(center_names, vectors_name)\n",
    "    city_coords= np.loadtxt(city_center)\n",
    "    vecs= np.loadtxt(vectors_name)\n",
    "    vecs = vecs.reshape((centers_count, 2, count))\n",
    "    fields= find_vector_fields_rbf_pandas(city_coords, vecs)\n",
    "    #save the field for all days/weeks/months\n",
    "    fields.to_csv(save_name) \n",
    "    return fields\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RBF function\n",
    "def init_parameters(dc):\n",
    "    min_map_x=min(list(dc['geometry'][547].exterior.coords.xy)[0]) \n",
    "    max_map_x=max(list(dc['geometry'][704].exterior.coords.xy)[0]) \n",
    "    min_map_y=min(list(dc['geometry'][171].exterior.coords.xy)[1]) \n",
    "    max_map_y=max(list(dc['geometry'][164].exterior.coords.xy)[1]) \n",
    "    global h, w\n",
    "    h=int((max_map_y-min_map_y)*10)\n",
    "    w=int((max_map_x-min_map_x)*10)\n",
    "    #قبل اینکه کل دیتا ست و مرکز شهرها رو ترنسفورم کنیم ایکس نقشه از وای نقشه بزرگتر بود ولی بعد از ترنسفورم  وای نقشه از ایکس بزرگتر است \n",
    "\n",
    "def rbf_interpolate(x_known, y_known, z_known):\n",
    "    width = np.linspace(0, w - 1, w)   #range -a to +a\n",
    "    height = np.linspace(0, h - 1,h)\n",
    "    x, y = np.meshgrid(height, width)\n",
    "    #print(x)\n",
    "    #print(y)\n",
    "    rbf = Rbf(x_known, y_known, z_known, epsilon=3, function='gaussian')\n",
    "    #######plot function \n",
    "    return rbf(x, y)\n",
    "\n",
    "\n",
    "def interpolate_field(centers, vectors):\n",
    "    x_known = centers[:, 0]\n",
    "    y_known = centers[:, 1]\n",
    "    u = rbf_interpolate(x_known, y_known, vectors[:, 0])\n",
    "    v = rbf_interpolate(x_known, y_known, vectors[:, 1])\n",
    "    return np.dstack([u, v])\n",
    "\n",
    "\n",
    "#find field for all 365 day/or 52-week/or 12-month\n",
    "def find_vector_fields_rbf(city_coords, vecs):\n",
    "    fields = np.zeros((count, w, h, 2))\n",
    "    for i in range(count):\n",
    "        fields[i] = interpolate_field(city_coords, vecs[:,:, i]) \n",
    "        print('field ' + str(i + 1) + ' found')\n",
    "    return fields\n",
    "\n",
    "def find_and_save_fields_rbf(city_center, vectors_name, save_name):\n",
    "    #city_coords, vecs = init_vecs(center_names, vectors_name)\n",
    "    city_coords= np.loadtxt(city_center)\n",
    "    vecs= np.loadtxt(vectors_name)\n",
    "    vecs = vecs.reshape((centers_count, 2, count))\n",
    "    fields= find_vector_fields_rbf(city_coords, vecs)\n",
    "    \n",
    "    #save the field for all days/weeks/months\n",
    "    saved = fields.reshape(count * 2 * h * w)\n",
    "    np.savetxt(save_name, saved)\n",
    "    return fields\n",
    "\n",
    "    \n",
    "def load_fields_rbf(name):\n",
    "    fields = np.loadtxt(name)\n",
    "    fields = fields.reshape((count, w, h, 2))\n",
    "    return fields\n",
    "\n",
    "\n",
    "def magnitude(vector):\n",
    "    r=vector*vector\n",
    "    r=r.sum(axis = len(np.shape(vector))-1)\n",
    "    r=np.sqrt(r)\n",
    "    return r\n",
    "\n",
    "\n",
    "def normalize_f(vector):\n",
    "    r=magnitude(vector)\n",
    "    rn=r.repeat(2).reshape((vector.shape[0],vector.shape[1],2))\n",
    "    rn=vector/rn\n",
    "    d=1\n",
    "    rn=rn*d\n",
    "    return rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation Ermia (tranformed data) - checked and confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the function of find and save fields and zeros compatible to new dataset wich is already transformed and scaled\n",
    "def init_xy(h, w): #h \n",
    "    global xy\n",
    "    idw = np.linspace(0, w - 1, w)\n",
    "    idh = np.linspace(0, h - 1, h)\n",
    "    x, y = np.meshgrid(idw, idh)\n",
    "    x, y = x.astype(int), y.astype(int)\n",
    "    xy = np.dstack([x, y])\n",
    "    \n",
    "    \n",
    "def add_corners1(coords, vecs):\n",
    "    coords = np.insert(coords, 0, [[0, 0], [0, w], [h, 0], [h, w]], axis=0)\n",
    "    vecs = np.insert(vecs, 0, [[[0] * count] * 2] * 4, axis=0)\n",
    "    return coords, vecs\n",
    "\n",
    "#assign the vector to each city center\n",
    "#centers: the coorrination of the center pof cities or microregions. for example file name: 'vector_center_city.txt'\n",
    "def init_vecs1(centers, vectors):\n",
    "    #centers:\n",
    "    city_coords = np.loadtxt(centers)\n",
    "    vecs = np.loadtxt(vectors)\n",
    "    vecs = vecs.reshape((centers_count, 2, count))\n",
    "    \n",
    "    return city_coords, vecs\n",
    "\n",
    "# make triangles\n",
    "def interpolate(center, corners, vectors):\n",
    "    d0 = np.cross(corners[1] - center, corners[2] - center) / np.linalg.norm(corners[1] - corners[2])\n",
    "    d1 = np.cross(corners[0] - center, corners[2] - center) / np.linalg.norm(corners[0] - corners[2])\n",
    "    d2 = np.cross(corners[1] - center, corners[0] - center) / np.linalg.norm(corners[1] - corners[0])    \n",
    "    \n",
    "    if (d0 + d1 + d2 == 0):\n",
    "        return np.array([0, 0])\n",
    "    \n",
    "    s = np.array([d0, d1, d2]) / (d0 + d1 + d2)\n",
    "    return s[0] * vectors[0] + s[1] * vectors[1] + s[2] * vectors[2] \n",
    "\n",
    "#find the vectors for all points (centers of the cell)\n",
    "#function for one special field for example daily, monthly, seasonly\n",
    "def find_field(city_coords, city_vecs):\n",
    "    tri = Delaunay(city_coords)\n",
    "    labels = tri.find_simplex(xy)\n",
    "    vertices = tri.simplices\n",
    "    rounds = vertices[labels]\n",
    "    field = np.zeros((h, w, 2))\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            field[i, j] = interpolate(xy[i, j], city_coords[rounds[i, j]], city_vecs[rounds[i, j]])\n",
    "    return field\n",
    "\n",
    "#find field for all 365 day/or 52-week/or 12-month\n",
    "def find_vector_fields(city_coords, vecs):\n",
    "    fields = np.zeros((count, h, w, 2))\n",
    "    for i in range(count):\n",
    "        fields[i] = find_field(city_coords, vecs[:,:, i])\n",
    "        print('field ' + str(i + 1) + ' found')\n",
    "    return fields\n",
    "\n",
    "\n",
    "def find_and_save_fields(center_names, vectors_name, save_name):\n",
    "    city_coords, vecs = init_vecs1(center_names, vectors_name)\n",
    "    city_coords, vecs = add_corners1( city_coords, vecs)\n",
    "    fields = find_vector_fields(city_coords, vecs)\n",
    "    #save the field for all days/weeks/months\n",
    "    saved = fields.reshape(count * 2 * h * w)\n",
    "    np.savetxt(save_name, saved)\n",
    "    return fields\n",
    "        \n",
    "        \n",
    "        \n",
    "def initialize():#\n",
    "    global h, w, count, centers_count\n",
    "    \n",
    "    #this below should be modified\n",
    "    \n",
    "    mg_map_transform= gpd.read_file('mg_gps_map_data/mg_map_transform/mg_map_transform.shp')\n",
    "    mg_map_transform['center_co']=mg_map_transform['geometry'].centroid\n",
    "    \n",
    "    \n",
    "    min_map_x=min(list(mg_map_transform['geometry'][547].exterior.coords.xy)[1]) \n",
    "    max_map_x=max(list(mg_map_transform['geometry'][704].exterior.coords.xy)[1]) \n",
    "    min_map_y=min(list(mg_map_transform['geometry'][171].exterior.coords.xy)[0]) \n",
    "    max_map_y=max(list(mg_map_transform['geometry'][164].exterior.coords.xy)[0])\n",
    "    \n",
    "    #this above should be modified\n",
    "    #h= int(max_map_y-min_map_y)\n",
    "    #w =int(max_map_x-min_map_x)\n",
    "    #w, h = int(11.189252920000001*100) , int(8.672043336000002*100) \n",
    "    w, h = int(11.189252920000001*10) , int(8.672043336000002*10)\n",
    "    init_xy(h, w)\n",
    "    \n",
    "    count = 12\n",
    "    #change number of cities/mics here\n",
    "    centers_count = 853 \n",
    "    \n",
    "    \n",
    "def load_fields(name):\n",
    "    initialize()\n",
    "    fields = np.loadtxt(name)\n",
    "    fields = fields.reshape((count, h, w, 2))\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check if point inside the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_inside_map():\n",
    "    from shapely.geometry import Point\n",
    "    mg_boundary= gpd.read_file('mg_gps_map_data/MG_total_boundry/th878nx5786.shp') \n",
    "    polygon_boundry= mg_boundary['geometry'][0]\n",
    "    x_y_in=[]\n",
    "\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        for j in range(np.shape(x)[1]):\n",
    "            point=Point(x[i,j],y[i,j])\n",
    "            if polygon_boundry.contains(point) is True:\n",
    "                x_y_in.append((x[i,j],y[i,j]))\n",
    "    return x_y_in\n",
    "\n",
    "def point_inside_transformed_map(p_x, p_y):\n",
    "    from shapely.geometry import Point\n",
    "    \n",
    "    #mg_boundary= gpd.read_file('mg_gps_map_data/Mg_total_transform/mg_total(boundry)_transform.shp') \n",
    "    mg_boundary= gpd.read_file('mg_gps_map_data/Mg_total_transform_87_118/mg_total(boundry)_transform_87_118.shp') \n",
    "    \n",
    "    polygon_boundry= mg_boundary['geometry'][0]\n",
    "    point=Point(p_x,p_y)\n",
    "    if polygon_boundry.contains(point) is True:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find sinks and sources and other types of critical points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sink(jacob,threshold_jacob):\n",
    "    trace = jacob[0, 0] + jacob[1, 1]\n",
    "    det = jacob[0, 0] * jacob[1, 1] - jacob[0, 1] * jacob[1, 0]\n",
    "    return trace > threshold_jacob and det > threshold_jacob\n",
    "\n",
    "\n",
    "def jacobian(field, point):\n",
    "    jacob = np.array([[0.0, 0.0], [0.0, 0.0]])\n",
    "    x, y = point\n",
    "    jacob[0, 0] = field[x + 1, y, 0] - field[x - 1, y, 0] \n",
    "    jacob[1, 0] = field[x + 1, y, 1] - field[x - 1, y, 1] \n",
    "    jacob[0, 1] = field[x, y + 1, 0] - field[x, y - 1, 0]\n",
    "    jacob[1, 1] = field[x, y + 1, 1] - field[x, y - 1, 1]\n",
    "    return jacob\n",
    "\n",
    "\n",
    "def find_critical_points(field, threshold):\n",
    "    magnitude = np.sqrt(field[:, :, 0]**2 + field[:, :, 1]**2)\n",
    "    indices = np.where(magnitude[1:-1, 1:-1] < threshold)\n",
    "    return np.dstack([indices[0], indices[1]])[0] + [1, 1]\n",
    "\n",
    "\n",
    "def find_sinks(field):\n",
    "    critical_points = find_critical_points(field, 1)\n",
    "    sinks = []\n",
    "    for point in critical_points:\n",
    "        jacob = jacobian(field, point)\n",
    "        if is_sink(jacob,0.01):\n",
    "            sinks.append(point)\n",
    "    return sinks\n",
    "\n",
    "def is_source(jacob,threshold_jacob):\n",
    "    trace = jacob[0, 0] + jacob[1, 1]\n",
    "    det = jacob[0, 0] * jacob[1, 1] - jacob[0, 1] * jacob[1, 0]\n",
    "    return trace < -threshold_jacob and det < -threshold_jacob\n",
    "\n",
    "def find_sources(field):\n",
    "    critical_points = find_critical_points(field, 1)\n",
    "    sources = []\n",
    "    for point in critical_points:\n",
    "        jacob = jacobian(field, point)\n",
    "        if is_source(jacob,0.01):\n",
    "            sources.append(point)\n",
    "    return sources\n",
    "\n",
    "\n",
    "\n",
    "#find three structures: Repelling focus, Attracting focus, and center\n",
    "def is_contour(jacob,threshold_jacob):\n",
    "    trace = jacob[0, 0] + jacob[1, 1]\n",
    "    det = jacob[0, 0] * jacob[1, 1] - jacob[0, 1] * jacob[1, 0]\n",
    "    return trace*trace - 4*det < -threshold_jacob \n",
    "\n",
    "    \n",
    "    \n",
    "def find_contours(field):\n",
    "    critical_points = find_critical_points(field, 1)\n",
    "    contours = []\n",
    "    for point in critical_points:\n",
    "        jacob = jacobian(field, point)\n",
    "        if is_contour(jacob,0.01):\n",
    "            contours.append(point)\n",
    "    return contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove all the vectors and values outside the map borders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zero_values_outside_map_border(fields):\n",
    "    field_insd= np.zeros(np.shape(fields))\n",
    "    for i in range(np.shape(field_insd)[0]):\n",
    "        for xi in range(np.shape(field_insd)[1]):\n",
    "            for yi in range(np.shape(field_insd)[2]):\n",
    "                if point_inside_transformed_map(xi, yi)==1:\n",
    "                    field_insd[i,xi,yi]= fields[i,xi,yi]\n",
    "                else:\n",
    "                    field_insd[i,xi,yi]= np.array([0,0])\n",
    "    return field_insd\n",
    "\n",
    "def make_none_values_outside_map_border(fields):\n",
    "    field_insd= np.zeros(np.shape(fields))\n",
    "    for i in range(np.shape(field_insd)[0]):\n",
    "        for xi in range(np.shape(field_insd)[1]):\n",
    "            for yi in range(np.shape(field_insd)[2]):\n",
    "                if point_inside_transformed_map(xi, yi)==1:\n",
    "                    field_insd[i,xi,yi]= fields[i,xi,yi]\n",
    "                else:\n",
    "                    field_insd[i,xi,yi]= None\n",
    "    return field_insd\n",
    "\n",
    "def make_field_matrix_symmetric(field_insd):\n",
    "    field_sym= np.copy(field_insd)\n",
    "    dif= abs(np.shape(field_sym)[2]-np.shape(field_sym)[1])\n",
    "    #dif is an even number we need to add one row more to one of the sides\n",
    "    #make the row we need to add ready:\n",
    "    z_row_r = np.zeros((np.shape( field_sym)[0], np.shape( field_sym)[1],int((dif+1)/2), np.shape(field_sym)[3]))\n",
    "    z_row_l = np.zeros((np.shape( field_sym)[0], np.shape( field_sym)[1],int((dif-1)/2), np.shape(field_sym)[3]))\n",
    "    #add half of the zero columns +1 to the right\n",
    "    field_sym = np.concatenate((z_row_r, field_sym), axis=2)\n",
    "    #add half of the zero columns to the left\n",
    "    field_sym = np.concatenate(( field_sym, z_row_l), axis=2)\n",
    "    \n",
    "    return field_sym\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# portrait divergence function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(G, firstnode, numberOfstep ):\n",
    "    matchedPath = []\n",
    "    n_step=1\n",
    "    currNode= firstnode\n",
    "    matchedPath.append(currNode)\n",
    "\n",
    "    while n_step <= numberOfstep:\n",
    "        connectNodes = [x[1] for x in G.out_edges(currNode,data = True)]\n",
    "        connectWeights = [x[2]['weight'] for x in G.out_edges(currNode,data = True)]\n",
    "        if len(connectNodes)==0:\n",
    "            n_step= numberOfstep+1\n",
    "        else:\n",
    "            remainingWeights = [z/sum(connectWeights) for z in connectWeights]\n",
    "\n",
    "            nextNode_index = np.random.choice(len(connectNodes), 1, p=remainingWeights)\n",
    "            nextNode= connectNodes[nextNode_index[0]] \n",
    "            matchedPath.append(nextNode)\n",
    "            currNode = nextNode\n",
    "            n_step+=1\n",
    "        \n",
    "    return matchedPath\n",
    "\n",
    "def random_walk_noweight(G, firstnode, numberOfstep ):\n",
    "    matchedPath = []\n",
    "    n_step=1\n",
    "    currNode= firstnode\n",
    "    matchedPath.append(currNode)\n",
    "\n",
    "    while n_step <= numberOfstep:\n",
    "        connectNodes = [x[1] for x in G.out_edges(currNode,data = True)]\n",
    "        if len(connectNodes)==0:\n",
    "            n_step= numberOfstep+1\n",
    "        else:\n",
    "            nextNode_index = np.random.choice(len(connectNodes), 1)\n",
    "            nextNode= connectNodes[nextNode_index[0]] \n",
    "            matchedPath.append(nextNode)\n",
    "            currNode = nextNode\n",
    "            n_step+=1\n",
    "        \n",
    "    return matchedPath\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate cossine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
